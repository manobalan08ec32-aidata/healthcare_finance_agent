"""
Prompt A Builder: SQL Generation with History Reference
Used when similarity_score >= 75 (called by parent function)

Token budget: ~1850 tokens for rules + metadata/history SQL
"""

from datetime import datetime


def build_prompt_A_with_history(
    current_question: str,
    history_question: str,
    history_sql: str,
    similarity_score: int,
    score_reasoning: str,
    metadata: str,
    mandatory_filters: str,
    filter_context: str,
    join_clause: str = ""
) -> str:
    """
    Build SQL generation prompt when history SQL is available and score >= 75.
    Uses metadata as source of truth, history SQL as structural reference.
    """
    
    # Determine score tier for action guidance
    if similarity_score >= 95:
        score_tier = "CLONE_AND_SWAP"
        tier_note = "Very high similarity. Clone SQL and swap filter values if columns validate."
    else:  # 75-94
        score_tier = "STRUCTURAL_TEMPLATE"
        tier_note = "High similarity. Inherit patterns from history, build columns from metadata."
    
    prompt = f"""You are a Databricks SQL generator. Use history SQL as reference, metadata as source of truth.

CURRENT QUESTION: {current_question}

SIMILARITY: {similarity_score}/100 ({score_tier})
{tier_note}
MATCH REASON: {score_reasoning}

HISTORY REFERENCE
Question: {history_question}
<history_sql>
{history_sql}
</history_sql>

METADATA (Source of Truth)
{metadata}

MANDATORY FILTERS
{mandatory_filters}

FILTER VALUES
{filter_context if filter_context else "None"}
{f"JOIN: {join_clause}" if join_clause else ""}

STEP 1: COLUMN VALIDATION

Before using history SQL, validate every column against METADATA.

For each column in history SQL (SELECT, WHERE, GROUP BY):
- EXISTS: Column found in metadata, use directly
- EQUIVALENT: Not found but similar meaning exists in metadata, map to it
  Examples: revenue_amount to total_revenue, carrier_id to carrier_code
- NOT_FOUND: No match in metadata, must find alternative

Validation outcome:
- All EXISTS/EQUIVALENT in SELECT and GROUP BY: Proceed with score-based action
- Any NOT_FOUND in SELECT or GROUP BY: Downgrade to STRUCTURAL_TEMPLATE
- NOT_FOUND in WHERE only: Skip that filter, continue

STEP 2: SCORE-BASED ACTION

SCORE 95-100 with all columns validated: CLONE_AND_SWAP

This is mechanical, not creative:
1. Copy history SQL exactly
2. Replace ONLY:
   - Filter values in WHERE (PBM to HDP, Specialty to Mail)
   - Date values (month=7 to month=8, Q3 to Q4)
   - Entity names if changed
   - EQUIVALENT columns to their metadata names
3. DO NOT change:
   - SELECT structure, column order, aliases
   - GROUP BY columns (except equivalent swaps)
   - Calculations, CASE WHEN logic
   - ORDER BY, LIMIT, HAVING

If any SELECT/GROUP BY column is NOT_FOUND, downgrade to STRUCTURAL_TEMPLATE.

SCORE 75-94 or downgraded: STRUCTURAL_TEMPLATE

INHERIT patterns from history:
- CASE WHEN for side-by-side time comparisons
- UNION/UNION ALL structure if present
- CTE/subquery architecture
- Aggregation style (SUM/COUNT placement)
- ROUND, NULLIF, UPPER patterns
- Aliasing conventions

BUILD from metadata:
- SELECT: Find correct columns from metadata for current question
- GROUP BY: Based on current question dimensions
- WHERE: Current filter values with mandatory filters
- Every column must exist in metadata

STEP 3: SQL RULES (Always Apply)

M1. Mandatory filters MUST be in WHERE clause

M2. String comparison always case-insensitive:
WHERE UPPER(column) = UPPER('value')

M3. Show calculation components:
SELECT SUM(revenue) as revenue, SUM(cost) as cost, SUM(revenue)-SUM(cost) as margin

M4. For derived metrics use CASE WHEN, not GROUP BY metric_type:
SELECT dimension,
  SUM(CASE WHEN UPPER(metric_type)=UPPER('Revenue') THEN amount ELSE 0 END) as revenue,
  SUM(CASE WHEN UPPER(metric_type)=UPPER('Cost') THEN amount ELSE 0 END) as cost
FROM table
GROUP BY dimension

M5. Formatting:
- Amounts: ROUND(x, 0)
- Percentages: ROUND(x, 3)  
- Division: x / NULLIF(y, 0)

M6. Final verification: Every column in output SQL must exist in METADATA

STEP 4: OUTPUT

COLUMN VALIDATION:
[List each history column: EXISTS | EQUIVALENT(mapped_name) | NOT_FOUND]

ACTION: CLONE_AND_SWAP | STRUCTURAL_TEMPLATE
REASON: [One line explaining what you did]

<sql>
[SQL query with all columns verified against metadata]
</sql>
<history_sql_used>true</history_sql_used>

If clarification needed:
<followup>
[Specific question]
Available columns: [from metadata]
</followup>"""

    return prompt


"""
MINIMAL CHANGES to add similarity_score to your existing _llm_feedback_selection

Your current design works well. These are the ONLY changes needed:
1. Add similarity_score to the LLM output format in the prompt
2. Add similarity_score to the return dict
3. Map confidence to score range

DO NOT replace your entire method. Just make these targeted changes.
"""

# =============================================================================
# CHANGE 1: Update the OUTPUT FORMAT section in your system_prompt
# =============================================================================

# FIND this in your current system_prompt (around line 85-95):
"""
OUTPUT FORMAT

Return ONLY JSON in <json> tags. No explanation outside tags.

<json>
{
  "status": "match_found" or "no_match",
  "selected_seq_id": <number> or null,
  "matched_question": "<text>" or null,
  "table_name": "<name>" or null,
  "reasoning": "<1-line explanation with tier>",
  "match_confidence": "HIGH" or "MEDIUM" or "LOW"
}
</json>

Confidence: HIGH=TIER A | MEDIUM=TIER B | LOW=TIER C
"""

# REPLACE WITH:
"""
OUTPUT FORMAT

Return ONLY JSON in <json> tags. No explanation outside tags.

<json>
{
  "status": "match_found" or "no_match",
  "selected_seq_id": <number> or null,
  "matched_question": "<text>" or null,
  "table_name": "<name>" or null,
  "reasoning": "<1-line explanation with tier>",
  "match_confidence": "HIGH" or "MEDIUM" or "LOW",
  "similarity_score": <number 0-100>
}
</json>

Confidence and Score mapping:
HIGH = TIER A = score 95-100 (only filter/time differs)
MEDIUM = TIER B = score 75-94 (same metric, structural patterns reusable)
LOW = TIER C = score 40-74 (minimal reuse value)
NO_MATCH = score 0-39
"""


# =============================================================================
# CHANGE 2: Add similarity_score to your return dicts
# =============================================================================

# FIND your match_found return block (around line 135-147):
"""
return {
    'status': 'match_found',
    'seq_id': selection_result.get('selected_seq_id'),
    'question': selection_result.get('matched_question'),
    'table_name': selection_result.get('table_name'),
    'reasoning': selection_result.get('reasoning'),
    'pattern_match_level': selection_result.get('pattern_match_level'),
    'error': False,
    'error_message': ''
}
"""

# REPLACE WITH (just add similarity_score):
"""
return {
    'status': 'match_found',
    'seq_id': selection_result.get('selected_seq_id'),
    'question': selection_result.get('matched_question'),
    'table_name': selection_result.get('table_name'),
    'reasoning': selection_result.get('reasoning'),
    'pattern_match_level': selection_result.get('pattern_match_level'),
    'match_confidence': selection_result.get('match_confidence'),
    'similarity_score': selection_result.get('similarity_score', 0),
    'error': False,
    'error_message': ''
}
"""

# FIND your no_match return block (around line 150-160):
"""
return {
    'status': 'no_match',
    'seq_id': None,
    'question': None,
    'table_name': None,
    'reasoning': selection_result.get('reasoning'),
    'pattern_match_level': 'NONE',
    'error': False,
    'error_message': ''
}
"""

# REPLACE WITH (just add similarity_score):
"""
return {
    'status': 'no_match',
    'seq_id': None,
    'question': None,
    'table_name': None,
    'reasoning': selection_result.get('reasoning'),
    'pattern_match_level': 'NONE',
    'match_confidence': 'NONE',
    'similarity_score': selection_result.get('similarity_score', 0),
    'error': False,
    'error_message': ''
}
"""


# =============================================================================
# CHANGE 3: Update error return blocks to include similarity_score
# =============================================================================

# Add similarity_score: 0 to all your error return blocks
# Example for JSON parse error block:
"""
return {
    'status': 'no_match',
    'seq_id': None,
    'question': None,
    'table_name': None,
    'reasoning': f"Failed to parse LLM response after {max_retries} attempts: {str(e)}",
    'pattern_match_level': 'NONE',
    'match_confidence': 'NONE',
    'similarity_score': 0,
    'error': True,
    'error_message': f"JSON parsing failed after {max_retries} attempts"
}
"""


# =============================================================================
# CHANGE 4: In _assess_and_generate_sql_async, extract and use the score
# =============================================================================

# FIND where you process feedback_selection_result (around line 26-46):
"""
if feedback_selection_result.get('status') == 'match_found':
    matched_seq_id = feedback_selection_result.get('seq_id')
    
    matched_record = None
    for result in feedback_results:
        if result.get('seq_id') == matched_seq_id:
            matched_record = result
            break
    
    if matched_record:
        history_question_match = matched_record.get('user_question', '')
        matched_sql = matched_record.get('sql_query', '')
        matched_table_name = matched_record.get('table_name', '')
        
        state['history_question_match'] = history_question_match
        state['matched_sql'] = matched_sql
        state['matched_table_name'] = matched_table_name
"""

# ADD these lines after state assignments:
"""
        # Extract similarity score for prompt selection
        similarity_score = feedback_selection_result.get('similarity_score', 0)
        score_reasoning = feedback_selection_result.get('reasoning', '')
        match_confidence = feedback_selection_result.get('match_confidence', 'LOW')
        
        state['similarity_score'] = similarity_score
        state['score_reasoning'] = score_reasoning
        state['match_confidence'] = match_confidence
        
        print(f"Similarity score: {similarity_score}, Confidence: {match_confidence}")
"""


# =============================================================================
# CHANGE 5: Use score to decide prompt (in _assess_and_generate_sql_async)
# =============================================================================

# FIND where you check has_history (around line 58):
"""
has_history = bool(matched_sql and history_question_match and matched_table_name)
"""

# ADD below it:
"""
similarity_score = state.get('similarity_score', 0)

# Use Prompt A (with history) only if score >= 70
has_usable_history = has_history and similarity_score >= 70
"""

# Then use has_usable_history to decide which prompt to build
