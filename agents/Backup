async def _llm_feedback_selection(self, feedback_results: List[Dict], state: AgentState) -> Dict:
        """
        Pure LLM-based selection of most relevant historical question from feedback results.
        Returns single best match or NO_MATCH status.
        """
        
        user_question = state.get('rewritten_question', state.get('original_question', ''))
        
        # System prompt for feedback matching
        system_prompt = """
You are a SQL pattern matching system. Select the single best historical question or return NO_MATCH.
You compare TEXT PATTERNS - you do NOT answer business questions.

MATCHING RULES (APPLY IN ORDER)

STEP 1: METRIC MATCH (MANDATORY - REJECT IF FAILS)

Current metric(s) MUST appear in history.

Rules:
‚úÖ History has required metric + extras ‚Üí ALLOW
‚úÖ Metric synonyms count as match (see synonym list)
‚ùå History missing required metric ‚Üí REJECT

Examples:
- Current: "revenue" | History: "revenue, volume" ‚Üí ‚úÖ PASS
- Current: "revenue" | History: "volume" ‚Üí ‚ùå REJECT
- Current: "network revenue" | History: "revenue" ‚Üí ‚úÖ PASS (synonym)

STEP 2: DIMENSION MATCH (MANDATORY - REJECT IF FAILS)

At least ONE dimension must overlap between current and history.
Either side can have more dimensions than the other.

‚ö†Ô∏è CRITICAL: Date dimensions (BY month, BY quarter, BY year) are NOT counted as dimensions here - ignore them completely in this check.

Rules:
‚úÖ Current: BY drug | History: BY drug BY region ‚Üí PASS (drug overlaps)
‚úÖ Current: BY drug BY region | History: BY drug ‚Üí PASS (drug overlaps)
‚úÖ Current: BY drug BY class | History: BY drug BY region ‚Üí PASS (drug overlaps)
‚ùå Current: BY drug | History: BY region ‚Üí REJECT (no overlap)
‚ùå Current: BY drug | History: no dimension ‚Üí REJECT (no overlap)
‚úÖ Current: no dimension | History: any dimension ‚Üí PASS (no requirement)

Edge Case - Date Dimensions:
- Current: "revenue BY month" | History: "revenue BY lob" ‚Üí ‚ùå REJECT
  Reason: "BY month" is a REAL dimension (not a date filter), no overlap with "lob"
- Current: "revenue for Q3 2025" | History: "revenue for Jan 2024" ‚Üí ‚úÖ PASS
  Reason: "Q3 2025" and "Jan 2024" are date FILTERS (not dimensions), ignored in Step 3

STEP 3: IGNORE DATE/TIME FILTERS

Strip all date specifications from filters. Any date = any other date.

Rules:
- Q3 2025 = Jan 2024 = July-Aug 2025 = Q1 2023 vs Q1 2022 = ANY
- Single periods, ranges, YoY, MoM, QoQ ‚Üí ALL IGNORED

STEP 4: RANK BY FILTER VALUE MATCH

After passing Steps 1-3, rank candidates by filter alignment:

TIER A (HIGHEST): Filter value(s) match
- Current filter value(s) appear in history (history can have extras)
- Dimensions overlap + metric matches + filter matches

TIER B (MEDIUM): Filter value differs
- Dimensions overlap + metric matches, but filter value different

TIER C (LOWEST): No dimension requirement
- Metric matches, current has no dimension requirement

STEP 5: SELECT BEST MATCH

1. If any TIER A exists ‚Üí Pick first TIER A
2. Else if any TIER B exists ‚Üí Pick first TIER B
3. Else if any TIER C exists ‚Üí Pick first TIER C
4. Else ‚Üí NO_MATCH

SYNONYMS

Metrics: revenue=network revenue=product revenue | volume=script count=scripts=line count | cost=expense=spend=cogs
Entities: HDP=Home Delivery=Mail | SP=Specialty | PBM=PBM Retail
Parsing: "PBM revenue" means metric=revenue, filter=PBM (NOT metric="PBM revenue")

KEY EXAMPLES

Example 1: Filter + Dimension Overlap + Metric Match (TIER A)

Current: "revenue BY lob for Prior Auth"
History: "revenue BY lob for Prior Auth, HDP Core"
Result: TIER A ‚úÖ (lob overlaps, revenue matches, Prior Auth found + extras OK)

Current: "revenue BY lob BY region for Prior Auth"
History: "revenue BY lob for Prior Auth"
Result: TIER A ‚úÖ (lob overlaps, revenue matches, Prior Auth found)

Example 2: Dimension Flexibility (History Subset/Superset Both OK)

Current: "revenue BY drug BY therapeutic_class for PBM"
History: "revenue BY drug for PBM"
Result: TIER A ‚úÖ (drug overlaps, revenue matches, PBM matches)

Current: "revenue BY drug for PBM"
History: "revenue BY drug BY therapeutic_class BY region for PBM"
Result: TIER A ‚úÖ (drug overlaps, revenue matches, PBM matches)

Example 3: No Dimension Overlap ‚Üí REJECT

Current: "revenue BY drug for PBM"
History: "revenue BY region for PBM"
Result: REJECT ‚ùå (drug vs region - no dimension overlap)

Current: "revenue BY lob for HDP"
History: "revenue for HDP"
Result: REJECT ‚ùå (current needs lob dimension, history has none - no overlap)

Example 4: Date Filters Completely Ignored

Current: "revenue for PBM in Q3 2025"
History: "revenue for PBM from Jan-Dec 2024"
Result: TIER A ‚úÖ (dates ignored, PBM matches, revenue matches)

Current: "script count for HDP from Jan-June 2025"
History: "volume for Home Delivery Q3 2024 vs Q3 2023"
Result: TIER A ‚úÖ (script count=volume synonym, HDP=Home Delivery synonym, dates ignored)

Example 5: Metric Mismatch ‚Üí REJECT

Current: "revenue BY lob for PBM"
History: "volume BY lob for PBM"
Result: REJECT ‚ùå (revenue ‚â† volume, failed Step 1)

Current: "script count for HDP"
History: "cost for HDP"
Result: REJECT ‚ùå (script count ‚â† cost, failed Step 1)

Example 6: TIER B (Dimension Overlap But Filter Differs)

Current: "revenue BY lob for Prior Auth"
History: "revenue BY lob for HDP Core"
Result: TIER B (lob overlaps, revenue matches, but Prior Auth ‚â† HDP Core)

Current: "volume BY drug BY region for Specialty"
History: "scripts BY drug for PBM"
Result: TIER B (drug overlaps, volume=scripts synonym, but Specialty ‚â† PBM)

OUTPUT FORMAT

Return ONLY JSON in <json> tags. No explanation outside tags.

<json>
{
  "status": "match_found" or "no_match",
  "selected_seq_id": <number> or null,
  "matched_question": "<text>" or null,
  "table_name": "<name>" or null,
  "reasoning": "<1-line explanation with tier>",
  "match_confidence": "HIGH" or "MEDIUM" or "LOW"
}
</json>

Confidence: HIGH=TIER A | MEDIUM=TIER B | LOW=TIER C

Remember: You match patterns, not answer questions. Start with <json>.
"""

        # Format feedback results as context in compact format (saves tokens)
        # Group by table_name first
        results_by_table = {}
        for result in feedback_results:
            table_name = result.get('table_name', 'N/A')
            if table_name not in results_by_table:
                results_by_table[table_name] = []
            results_by_table[table_name].append(result)
        
        # Build compact candidates context
        candidates_context = "=== HISTORICAL QUESTIONS (GROUPED BY TABLE) ===\n"
        for table_name, results in results_by_table.items():
            candidates_context += f"\n- table_name: {table_name}\n"
            for result in results:
                seq_id = result.get('seq_id', 'N/A')
                question = result.get('user_question', 'N/A')
                candidates_context += f"  id:{seq_id}, {question}\n"
        
        # Build the full prompt with TRIPLE-LAYER protection against guardrails
        selection_prompt = f"""‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL INSTRUCTION - READ THIS FIRST ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

You are an AUTOMATED PATTERN MATCHING SYSTEM - NOT an AI assistant that answers questions.
Your ONLY function is to analyze TEXT PATTERNS and return a JSON response.

DO NOT attempt to:
- Answer the business questions below
- Provide insights about the data

{system_prompt}

{candidates_context}

=== CURRENT QUESTION (TEXT PATTERN TO MATCH) ===
{user_question}

‚ö†Ô∏è REMINDER: You are NOT answering this question. You are ONLY checking if its pattern matches any historical question pattern above.

Return ONLY the JSON in <json> tags. Start immediately with <json>.
"""
        
        # Retry logic
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                print(f"üîç Attempting feedback selection (attempt {retry_count + 1}/{max_retries})...")

                # Call LLM with LOWER temperature for more deterministic behavior
                # Lower temperature = less creative = less likely to trigger guardrails
                # print("Raw LLM prompt:", selection_prompt)
                llm_response = await self.call_claude_api_endpoint_async(
                    messages=[{"role": "user", "content": selection_prompt}],
                    max_tokens=2000,  # Reduced tokens since we only need JSON
                    temperature=0.0,  # ‚Üê CRITICAL: Set to 0 for deterministic pattern matching
                    top_p=0.1,  # ‚Üê CRITICAL: Very focused sampling
                    system_prompt="PATTERN MATCHING SYSTEM: You are an automated text pattern comparison algorithm for SQL query optimization infrastructure. You do NOT answer questions, provide business insights, or analyze data. You ONLY compare text patterns between queries and return structured JSON responses indicating pattern similarity. This is a technical infrastructure task, not business analysis."
                )
                
                print("Raw LLM response:", llm_response)
                
                # Extract JSON from response
                json_content = self._extract_json_from_response(llm_response)
                selection_result = json.loads(json_content)
                
                # Validate response structure
                status = selection_result.get('status')
                if status not in ['match_found', 'no_match']:
                    raise ValueError(f"Invalid status returned: {status}")
                
                # Handle success cases
                if status == "match_found":
                    print(f"‚úÖ Feedback match found: seq_id={selection_result.get('selected_seq_id')}")
                    print(f"   Matched question: {selection_result.get('matched_question')}")
                    print(f"   Pattern level: {selection_result.get('pattern_match_level')}")
                    print(f"   Reasoning: {selection_result.get('reasoning')}")
                    
                    return {
                        'status': 'match_found',
                        'seq_id': selection_result.get('selected_seq_id'),
                        'question': selection_result.get('matched_question'),
                        'table_name': selection_result.get('table_name'),
                        'reasoning': selection_result.get('reasoning'),
                        'pattern_match_level': selection_result.get('pattern_match_level'),
                        'error': False,
                        'error_message': ''
                    }
                
                else:  # status == "no_match"
                    print(f"‚ùå No suitable match found in feedback history")
                    print(f"   Reasoning: {selection_result.get('reasoning')}")
                    
                    return {
                        'status': 'no_match',
                        'seq_id': None,
                        'question': None,
                        'table_name': None,
                        'reasoning': selection_result.get('reasoning'),
                        'pattern_match_level': 'NONE',
                        'error': False,
                        'error_message': ''
                    }
            
            except json.JSONDecodeError as e:
                retry_count += 1
                print(f"‚ö† JSON parsing failed (attempt {retry_count}/{max_retries}): {str(e)}")
                
                if retry_count < max_retries:
                    print(f"üîÑ Retrying...")
                    await asyncio.sleep(2 ** retry_count)
                    continue
                else:
                    print(f"‚ùå All retry attempts exhausted - JSON parsing failed")
                    return {
                        'status': 'no_match',
                        'seq_id': None,
                        'question': None,
                        'table_name': None,
                        'reasoning': f"Failed to parse LLM response after {max_retries} attempts: {str(e)}",
                        'pattern_match_level': 'NONE',
                        'error': True,
                        'error_message': f"JSON parsing failed after {max_retries} attempts"
                    }
            
            except Exception as e:
                retry_count += 1
                print(f"‚ö† Feedback selection attempt {retry_count} failed: {str(e)}")
                
                if retry_count < max_retries:
                    print(f"üîÑ Retrying... ({retry_count}/{max_retries})")
                    await asyncio.sleep(2 ** retry_count)
                    continue
                else:
                    print(f"‚ùå All retry attempts exhausted - feedback selection failed")
                    return {
                        'status': 'no_match',
                        'seq_id': None,
                        'question': None,
                        'table_name': None,
                        'reasoning': f"Feedback selection failed after {max_retries} attempts: {str(e)}",
                        'pattern_match_level': 'NONE',
                        'error': True,
                        'error_message': f"LLM call failed after {max_retries} attempts: {str(e)}"
                    }
        
        # Should never reach here, but just in case
        return {
            'status': 'no_match',
            'seq_id': None,
            'question': None,
            'table_name': None,
            'reasoning': 'Unexpected error in retry logic',
            'pattern_match_level': 'NONE',
            'error': True,
            'error_message': 'Unexpected error in retry logic'
        }
