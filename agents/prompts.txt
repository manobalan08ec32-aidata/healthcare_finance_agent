selection_prompt = f"""
            You are a Dataset Identifier Agent. You have FIVE sequential tasks to complete.

            CURRENT QUESTION: {user_question}
            EXTRACTED COLUMNS WITH FILTER VALUES: {filter_values}
            {filter_metadata_text}
            AVAILABLE DATASETS: {search_results}

            A. **PHI/PII SECURITY CHECK**:
                - First, examine each dataset's "PHI_PII_Columns" field (if present)
                - Analyze the user's question to identify if they are requesting any PHI/PII information
                - PHI/PII includes: SSN, member IDs, personal identifiers, patient names, addresses, etc.
                - Check if the user's question mentions or implies access to columns listed in "PHI_PII_Columns"
                - If PHI/PII columns are requested, IMMEDIATELY return phi_found status (do not proceed to other checks)
                
            B. **METRICS & ATTRIBUTES CHECK**:
                - Extract requested metrics/measures and attributes/dimensions
                - Apply smart mapping with these rules:
                
                **TIER 1 - Direct Matches**: Exact column names
                **TIER 2 - Standard Healthcare Mapping**: 
                    * "therapies" → "therapy_class_name"
                    * "scripts" → "unadjusted_scripts/adjusted_scripts"  
                    * "drugs" → "drug_name"
                    * "clients" → "client_id/client_name"
                
                **TIER 3 - Mathematical Operations**: 
                    * "variance/variances" → calculated from existing metrics over time periods
                    * "growth/change" → period-over-period calculations
                    * "percentage/rate" → ratio calculations
                
                **TIER 4 - Skip Common Filter Values**: 
                    * Skip validation for: "external", "internal", "retail", "mail order", "commercial", "medicare", "brand", "generic"
                    * These appear to be filter values, not missing attributes
                
                **BLOCK - Creative Substitutions**:
                    * Do NOT map unrelated terms (e.g., "ingredient fee" ≠ "expense")
                    * Do NOT assume domain knowledge not in metadata

                - Only mark as missing if NO reasonable Tier 1-3 mapping exists
                
                **NEW - EXPLICIT ATTRIBUTE DETECTION**:
                - Scan the user's question for explicit attribute keywords(examples below):
                    * "carrier" → carrier_name, carrier_id
                    * "drug" → drug_name, drug_id
                    * "pharmacy" → pharmacy_name, pharmacy_id
                    * "therapy" → therapy_class_name, therapy_id
                    * "client" → client_name, client_id
                    * "manufacturer" → drug_manufctr_nm, manufacturer_name
                - Flag as "explicit_attribute_mentioned = true/false"
                - Store mapped column names for later filter disambiguation check
                - This detection is case-insensitive and looks for these keywords anywhere in the question

            C. **KEYWORD & SUITABILITY ANALYSIS**:
            - **KEYWORD MATCHING**: Look for domain keywords that indicate preferences:
            * "claim/claims" → indicates claim_transaction dataset relevance
            * "forecast/budget" → indicates actuals_vs_forecast dataset relevance  
            * "ledger" → indicates actuals_vs_forecast dataset relevance
            
            - **CRITICAL: SUITABILITY VALIDATION (HARD CONSTRAINTS)**:
            * **BLOCKING RULE**: If a dataset's "not_useful_for" field contains keywords/patterns that match the user's question, IMMEDIATELY EXCLUDE that dataset regardless of other factors
            * **POSITIVE VALIDATION**: Check if user's question aligns with "useful_for" field patterns
            * **Example Applications**:
              - User asks "top 10 clients by expense" → Ledger has "not_useful_for": ["client level expense"] → EXCLUDE ledger table completely
              - User asks "claim-level analysis" → Claims has "useful_for": ["claim-level financial analysis"] → PREFER claims table
            * **PRECEDENCE**: not_useful_for OVERRIDES metrics/attributes availability - even if a table has the columns, exclude it if explicitly marked as not suitable
            
            - Verify time_grains match user needs (daily vs monthly vs quarterly)
            - Note: Keywords indicate relevance but suitability constraints are MANDATORY

            D. **COMPLEMENTARY ANALYSIS CHECK**:
            - **PURPOSE**: Identify if multiple datasets together provide more complete analysis than any single dataset
            - **LOOK FOR THESE PATTERNS**:
            * Primary metric in one dataset + dimensional attributes in another (e.g., "ledger revenue" + "therapy breakdown")
            * Different analytical perspectives on same business question (e.g., actuals view + claims view)
            * One dataset provides core data, another provides breakdown/segmentation
            * Cross-dataset comparison needs (e.g., budget vs actual vs claims)
            * **BREAKDOWN ANALYSIS**: When question asks for metric breakdown by dimensions not available in the primary dataset

            - **EVALUATION CRITERIA**:
            * Single dataset with ALL metrics + attributes → SELECT IT
            * No single complete dataset → SELECT MULTIPLE if complementary
            * Primary metric in A + breakdown dimension in B → SELECT BOTH

            **KEY EXAMPLES**:
            - "top 10 drugs by revenue" → Claims table (has revenue + drug_name) NOT Ledger (missing drug_name)
            - "total revenue" → Ledger table (high_level_table tie-breaker when both have revenue)
            - "ledger revenue breakdown by drug" → Both tables (complementary: ledger revenue + claims drug_name)

            **CLARIFICATION vs COMPLEMENTARY**:
            - Ask clarification when: Same data available in multiple datasets with different contexts OR multiple columns in same table
            - Select multiple when: Different but compatible data needed from each dataset for complete analysis

            F. **FINAL DECISION LOGIC**:
            - **STEP 1**: Check results from sections A through D
            - **STEP 2**: MANDATORY Decision order:
            * **FIRST**: Apply suitability constraints - eliminate datasets with "not_useful_for" matches
            * **SECOND**: Validate complete coverage (metrics + attributes) on remaining datasets
            * **THIRD**: Single complete dataset → SELECT IT
            
            * **NEW - SMART FILTER DISAMBIGUATION CHECK**:
                **STEP 3A**: Check if filter values exist in multiple columns
                **STEP 3B**: Determine if follow-up is needed using this SIMPLIFIED logic:
                
                1. **Check for explicit attribute mention**:
                   - Was an attribute explicitly mentioned in the question? (from Section B detection)
                   - Examples: "Carrier", "drug", "pharmacy", "therapy", "client", "manufacturer", "plan"
                
                2. **Count matching columns in selected dataset**:
                   - From filter metadata, identify all columns containing the filter value
                   - Filter to only columns that exist in the selected dataset's attributes
                   - Store as: matching_columns_count
                
                3. **SIMPLE DECISION TREE**:
                   - IF explicit_attribute_mentioned = true → NO FOLLOW-UP (trust user's specification)
                   - IF explicit_attribute_mentioned = false:
                       * IF matching_columns_count = 1 → NO FOLLOW-UP (obvious choice)
                       * IF matching_columns_count > 1 → RETURN "needs_disambiguation"
                
                **Examples**:
                - "Carrier MPDOVA billed amount" + MPDOVA in [carrier_name, carrier_id, plan_name] → ✓ NO follow-up (user said "Carrier")
                - "MPDOVA billed amount" + MPDOVA in [carrier_name] only → ✓ NO follow-up (only 1 match)
                - "covid vaccine billed amount" + covid vaccine in [drug_name, pharmacy_name, therapy_class_name] → ❌ ASK which column (no explicit attribute, 3 matches)

            * **FOURTH**: No single complete → SELECT MULTIPLE if complementary
            * **FIFTH**: Multiple complete → Use traditional tie-breakers (keywords, high_level_table)
            * **SIXTH**: Still tied → RETURN "needs_disambiguation" and ask user to choose
            * **LAST**: No coverage OR unresolvable ambiguity → Report as missing items or request clarification
            
            **HIGH LEVEL TABLE PRIORITY RULE** (ONLY APPLIES DURING TIES):
            - **CRITICAL**: High-level table priority is ONLY used as a tie-breaker when multiple datasets have ALL required metrics AND attributes
            - **PRIMARY RULE**: ALWAYS validate that dataset has both required metrics AND required attributes FIRST
            - **HIGH LEVEL QUESTION INDICATORS**: Questions asking for summary metrics, totals, aggregates, or general overviews without specific breakdowns
            - **Examples of HIGH LEVEL**: "total revenue", "overall costs", "summary metrics", "high-level view", "aggregate performance", "what is the revenue", "show me costs"  
            - **Examples of NOT HIGH LEVEL**: "revenue breakdown by therapy", "costs by client", "detailed analysis", "revenue by drug category", "performance by region", "top drugs by revenue", "top clients by cost"
            - **VALIDATION FIRST RULE**: 
                * Step 1: Check if dataset has required metrics (revenue, cost, etc.)
                * Step 2: Check if dataset has required attributes/dimensions (drug_name, therapy_class_name, client_id, etc.)
                * Step 3: ONLY if multiple datasets pass Steps 1 & 2, then check "high_level_table": "True" as tie-breaker
            - **NEVER OVERRIDE RULE**: Never select high_level_table if it's missing required attributes, even for "high-level" questions

            ==============================
            DECISION CRITERIA
            ==============================

            **PHI_FOUND** IF:
            - User question requests or implies access to PHI/PII columns
            - Any columns mentioned in "PHI_PII_Columns" are being requested
            - Must be checked FIRST before other validations

            **PROCEED** (SELECT DATASET) IF:
            - **STANDARD PATH**: Dataset passes suitability validation (not blocked by "not_useful_for" field) AND all requested metrics/attributes have Tier 1-3 matches AND clear selection
            - Single dataset meets all requirements after suitability and coverage checks
            - Complementary datasets identified for complete coverage after all validations

            **MISSING_ITEMS** IF:
            - Required metrics/attributes don't have Tier 1-3 matches in any dataset
            - No suitable alternatives available after all validation steps

            **REQUEST_FOLLOW_UP** IF:
            - **PRIORITY 1 - DATASET AMBIGUITY**: 
                * Multiple datasets with conflicting contexts AND no clear preference from traditional validation
                * ALWAYS ask user to specify which table/dataset to use FIRST
            
            - **PRIORITY 2 - SMART FILTER COLUMN AMBIGUITY**: 
                * User did NOT explicitly mention an attribute (e.g., no "carrier", "drug", "pharmacy" keywords)
                * AND filter value exists in 2+ columns within the selected dataset
                * Example: "covid vaccine billed amount" where drug_name, pharmacy_name, therapy_class_name all have "covid vaccine"
                * ALWAYS list all matching columns with sample values and ask user to specify
                * NOTE: If user explicitly mentions attribute (e.g., "Carrier MPDOVA"), NO follow-up needed regardless of multiple matches

            ==============================
            ASSESSMENT FORMAT (BRIEF)
            ==============================

            **ASSESSMENT**: A:✓(no PHI) B:✓(metrics found) B-ATTR:✓(explicit attr OR single column) C:✓(suitability passed) D:✓(complementary) F:✓(clear selection)
            **DECISION**: PROCEED - [One sentence reasoning]

            Keep assessment ultra-brief:
            - Use checkmarks (✓) or X marks (❌) with 10 words max explanation in parentheses
            - **NEW B-ATTR**: ✓ means explicit attribute mentioned OR only 1 column match (no ambiguity), ❌ means no explicit attr AND multiple columns (disambiguation needed)
            - **CRITICAL for C (suitability)**: ✓ means no "not_useful_for" conflicts, ❌ means blocked by suitability constraints
            - Each area gets: "A:✓(brief reason)" or "A:❌(brief issue)"
            - Decision reasoning maximum 15 words
            - No detailed explanations or bullet points in assessment
            - Save detailed analysis for JSON selection_reasoning field

            ==============================
            RESPONSE FORMAT
            ==============================

            IMPORTANT: Keep assessment ultra-brief (1-2 lines max), then output ONLY valid JSON.

            RESPONSE FORMAT MUST be valid JSON. Do NOT include any extra text, markdown, or formatting. The response MUST NOT start with ```json and end with ```
            
            {{
                "status": "phi_found" | "success" | "missing_items" | "needs_disambiguation",
                "final_actual_tables": ["table_name_1","table_name2"] if status = success else [],
                "functional_names": ["functional_name"] if status = success else [],
                "tables_identified_for_clarification": ["table_1", "table_2"] if status = needs_disambiguation else [],
                "requires_clarification": true if status = needs_disambiguation else false,
                "selection_reasoning": "2-3 lines max explanation",
                "high_level_table_selected": true/false if status = success else null,
                "user_message": "message to user" if status = phi_found or missing_items else null,
                "clarification_question": "question to user" if status = needs_disambiguation else null,
                "selected_filter_context": "col name - [actual_column_name], sample values [all values from filter extract]" if column selected from filter context else null
            }}
                """


async def _validate_fixed_sql(self, fixed_sql: str, original_sql: str, error_msg: str) -> Dict[str, Any]:
    """
    Guardrail validation to prevent embarrassing SQL outputs
    Returns dict with success=False if guardrails are violated
    """
    forbidden_patterns = [
        'show tables',
        'show databases',
        'describe table',
        'describe ',
        'information_schema.tables',
        'information_schema.columns',
        'show schemas',
        'list tables'
    ]
    
    sql_lower = fixed_sql.lower().strip()
    
    # Check for forbidden patterns
    for pattern in forbidden_patterns:
        if pattern in sql_lower:
            print(f"🚫 GUARDRAIL VIOLATION: Blocked '{pattern}' in LLM response")
            print(f"❌ Original error was: {error_msg}")
            
            return {
                'success': False,
                'error': f"Guardrail triggered: LLM attempted to generate '{pattern}' instead of fixing the query. This is not allowed.",
                'violated_pattern': pattern,
                'original_sql': original_sql
            }
    
    # Additional check: if it's a table not found error, ensure the fixed SQL 
    # still references a table (not just a utility query)
    table_not_found_indicators = ['table not found', 'table does not exist', 'no such table', 'invalid table name']
    if any(indicator in error_msg.lower() for indicator in table_not_found_indicators):
        # Check if the fixed SQL is suspiciously short or doesn't contain FROM
        if len(sql_lower) < 20 or 'from' not in sql_lower:
            print(f"🚫 GUARDRAIL VIOLATION: Fixed SQL too short or missing FROM clause for table-not-found error")
            return {
                'success': False,
                'error': "Guardrail triggered: Invalid fix for table-not-found error. Fixed query must contain valid FROM clause.",
                'original_sql': original_sql
            }
    
    return {'success': True, 'fixed_sql': fixed_sql}


async def _fix_sql_with_llm_async(self, failed_sql: str, error_msg: str, errors_history: List[str], context: Dict) -> Dict[str, Any]:
    """Use LLM to fix SQL based on error with enhanced prompting and retry logic async"""

    history_text = "\n".join(errors_history) if errors_history else "No previous errors"
    current_question = context.get('current_question', '')
    dataset_metadata = context.get('dataset_metadata', '')

    fix_prompt = f"""
    You are an expert Databricks SQL developer. A SQL query has **FAILED** and needs to be **FIXED or REWRITTEN**.

    ==============================
    CONTEXT
    ==============================
    - ORIGINAL USER QUESTION: "{current_question}"
    - TABLE METADATA: {dataset_metadata}

    ==============================
    FAILURE DETAILS
    ==============================
    - FAILED SQL QUERY:
    ```sql
    {failed_sql}
    ```
    - ERROR MESSAGE: {error_msg}
    - PREVIOUS RETRY ERRORS: {history_text}

    ==============================
    CRITICAL ERROR INTERPRETATION RULES (FOLLOW EXACTLY)
    ==============================
    1. TIMEOUT / TRANSIENT EXECUTION ERRORS
        - If the ERROR MESSAGE indicates a timeout or transient execution condition (contains ANY of these case-insensitive substrings: 
           "timeout", "timed out", "cancelled due to timeout", "query exceeded", "network timeout", "request timed out", "socket timeout"),
           then DO NOT modify the SQL. Return the ORIGINAL FAILED SQL verbatim as the fixed version. (Root cause is environmental, not syntax.)
        - Still wrap it in <sql> tags exactly as required.

    2. COLUMN NOT FOUND / INVALID IDENTIFIER
        - If missing/invalid column error.
        - FIRST: If error text itself lists / hints alternative or available columns (patterns like "Did you mean", "Available columns", "Similar: colA, colB"), pick the best match to the ORIGINAL USER QUESTION intent from those suggestions (these override metadata if conflict).
        - ELSE: Select a replacement from TABLE METADATA (exact / case-insensitive / close semantic match). Never reuse the invalid name. Do not invent new columns.
        - Change only what is required; keep all other logic intact.

    3. TABLE NOT FOUND / TABLE DOES NOT EXIST
        ⚠️ CRITICAL PROHIBITION:
        - If the ERROR MESSAGE indicates a table does not exist (contains ANY of these case-insensitive substrings: "table not found", "table does not exist", "no such table", "invalid table name"):
        
        🚫 ABSOLUTELY FORBIDDEN - DO NOT GENERATE:
            - SHOW TABLES
            - SHOW DATABASES  
            - DESCRIBE TABLE
            - Information schema queries
            - Any query that lists or discovers tables
        
        ✅ ONLY ALLOWED OPTIONS:
            a) If TABLE METADATA contains a similarly named valid table → Replace with that exact table name
            b) If no valid alternative exists → Return ORIGINAL FAILED SQL unchanged with a SQL comment explaining why
        
        - The query MUST still attempt to answer the ORIGINAL USER QUESTION using only tables in TABLE METADATA

    4. OTHER ERROR TYPES (syntax, mismatched types, aggregation issues, grouping issues, function misuse, alias conflicts, etc.)
        - Rewrite or minimally adjust the SQL to resolve the issue while preserving the analytical intent of the ORIGINAL USER QUESTION.
        - Ensure any columns used are present in TABLE METADATA.
        - If a derived metric is implied, derive it transparently in SELECT with proper component columns.

    5. NEVER:
        - Never fabricate table or column names not present in metadata.
        - Never remove necessary GROUP BY columns required for non-aggregated selected columns.
        - Never switch to a different table unless clearly required to satisfy a missing valid column.
        - Never generate SHOW TABLES, DESCRIBE, or any schema discovery query.

    6. ALWAYS:
        - Preserve filters, joins, and calculation intent unless they reference invalid columns.
        - Use consistent casing and UPPER() comparisons for string equality.
        - Include replaced column(s) in SELECT list if they are used in filters or aggregations.

    ==============================
    EXAMPLE: TABLE NOT FOUND ERROR
    ==============================
    ❌ WRONG - THIS WILL BE REJECTED:
    <sql>
    SHOW TABLES;
    </sql>

    ✅ CORRECT - Return original with comment:
    <sql>
    -- Table 'sales_2024' not found in metadata. Returning original query.
    -- Available tables should be verified in TABLE METADATA.
    SELECT * FROM sales_2024 WHERE year = 2024;
    </sql>

    ✅ ALSO CORRECT - Use alternative from metadata:
    <sql>
    -- Replaced 'sales_2024' with 'sales_data' from available tables
    SELECT * FROM sales_data WHERE year = 2024;
    </sql>

    ==============================
    DECISION PATH (FOLLOW IN ORDER)
    ==============================
    IF timeout-related → return original query unchanged
    ELSE IF column-not-found → replace invalid column with valid one from metadata  
    ELSE IF table-not-found → use alternative from metadata OR return original with comment
    ELSE → fix syntax/logic while preserving intent

    ==============================
    FINAL VALIDATION BEFORE OUTPUT
    ==============================
    ✓ Does SQL contain SHOW, DESCRIBE, or INFORMATION_SCHEMA? → If YES, REWRITE
    ✓ Does SQL use only tables from TABLE METADATA? → If NO, return original
    ✓ Does SQL answer the ORIGINAL USER QUESTION? → If NO, revise
    ✓ Is it wrapped in <sql></sql> tags? → If NO, add them

    ==============================
    RESPONSE FORMAT
    ==============================
    Return ONLY the fixed SQL query wrapped in XML tags. No other text, explanations, or formatting.

    <sql>
    SELECT ...your fixed SQL here...
    </sql>
    """

    for attempt in range(self.max_retries):
        try:
            llm_response = await self.db_client.call_claude_api_endpoint_async([
                {"role": "user", "content": fix_prompt}
            ])
            print('sql fix prompt', fix_prompt)

            # Extract SQL from XML tags
            match = re.search(r'<sql>(.*?)</sql>', llm_response, re.DOTALL)
            if match:
                fixed_sql = match.group(1).strip()
                fixed_sql = fixed_sql.replace('`', '')  # Remove backticks

                if not fixed_sql:
                    raise ValueError("Empty fixed SQL query in XML response")

                # 🛡️ GUARDRAIL VALIDATION
                validation_result = await self._validate_fixed_sql(fixed_sql, failed_sql, error_msg)
                
                if not validation_result['success']:
                    # Guardrail violated - return error immediately
                    print(f"⛔ Guardrail check failed: {validation_result['error']}")
                    return validation_result
                
                # Validation passed
                return {
                    'success': True,
                    'fixed_sql': fixed_sql
                }
            else:
                raise ValueError("No SQL found in XML tags")

        except Exception as e:
            print(f"❌ SQL fix attempt {attempt + 1} failed: {str(e)}")

            if attempt < self.max_retries - 1:
                print(f"🔄 Retrying SQL fix... (Attempt {attempt + 1}/{self.max_retries})")
                await asyncio.sleep(2 ** attempt)

    return {
        'success': False,
        'error': f"SQL fix failed after {self.max_retries} attempts due to Model errors"
    }
