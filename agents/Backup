def _build_reasoning_prompt(self, context: Dict, state: Dict) -> str:
        """Build the reasoning and validation prompt (Call 1)
        
        This prompt handles:
        - Filter validation against metadata
        - Semantic mapping with confidence levels
        - Ambiguity detection
        - User hints/corrections
        - Intent detection for multiple values
        - Filter resolution with priority hierarchy
        """
        
        current_question = context.get('current_question', '')
        dataset_metadata = context.get('dataset_metadata', '')
        filter_metadata_results = state.get('filter_metadata_results', [])
        mandatory_columns_text = state.get('mandatory_columns_text', '')
        history_question_match = state.get('history_question_match', '')
        matched_sql = state.get('matched_sql', '')
        selected_datasets = state.get('selected_dataset', [])
        
        # Get table name for output
        table_name = selected_datasets[0] if selected_datasets else "unknown_table"
        
        # Light history reference for filter resolution hints only
        history_hint = ""
        if matched_sql and history_question_match:
            history_hint = f"""
HISTORY REFERENCE (for filter column resolution hints only - NOT for time filters):
Previous question: {history_question_match}
<historical_sql>
{matched_sql}
</historical_sql>
If current question has a filter value that appears in multiple columns, check which column history used for similar filter.
‚ö†Ô∏è EXCEPTION: Do NOT use history for year/month/quarter/date - always use current question's time.
"""

        prompt = f"""You are a SQL planning assistant for DANA (Data Analytics & Navigation Assistant).
Your job is to VALIDATE and MAP - not to GUESS or ASSUME.

CORE PRINCIPLES:

1. ONE FOLLOW-UP OPPORTUNITY
You have exactly ONE chance to ask a clarifying question.
- ANY ambiguity ‚Üí ASK NOW (don't assume)
- ANY unknown value ‚Üí ASK NOW (don't hallucinate)
- ANY uncertain mapping ‚Üí ASK NOW (don't guess)
BETTER TO ASK than to ASSUME WRONG.

2. TERM vs VALUE MAPPING
- TERMS (revenue, cost, count) ‚Üí Semantic matching OK
- VALUES (MDOVA, comcard, Specialty) ‚Üí EXACT match ONLY from EXTRACTED FILTERS or METADATA samples

3. ZERO ASSUMPTIONS
‚ùå NEVER assume value maps to column based on similarity ("comcard" ‚â† "Community Pharmacies")
‚ùå NEVER invent filters not in question (no time mentioned ‚â† add current month)
‚ùå NEVER guess when uncertain
‚úÖ ALWAYS validate against EXTRACTED FILTERS and METADATA

4. VALIDATION BEFORE OUTPUT
Every column must exist in METADATA.
Every value must come from EXTRACTED FILTERS or METADATA samples or be parseable date/number.

INPUTS:

CURRENT QUESTION: {current_question}

AVAILABLE METADATA:
{dataset_metadata}

MANDATORY FILTERS:
{mandatory_columns_text}

EXTRACTED FILTER VALUES:
{filter_metadata_results}
{history_hint}

STAGE 1: EXTRACT FROM QUESTION

STEP 1.1: USER HINTS (check FIRST)
Explicit guidance = HIGH CONFIDENCE override:
- "use carrier_id" ‚Üí Use that column
- "I mean X not Y" ‚Üí Use specified
- "ignore therapy class" ‚Üí Exclude

STEP 1.2: EXTRACT TERMS
Concepts mapping to columns: revenue, cost, margin, count, membership, scripts, carrier, product, category, client
SKIP: show, get, give, what, is, the, for, data

STEP 1.3: EXTRACT VALUES  
Specific data points: MDOVA, comcard, Specialty, HDP, July 2025, Q3, 2024
‚ö†Ô∏è CRITICAL: Check METADATA for synonym mappings (e.g., "Mail->Home Delivery", "SP->Specialty", "HDP->Home Delivery")

STEP 1.4: DETECT INTENT
- simple_aggregate: "what is total revenue"
- breakdown: "revenue by category" or "revenue for HDP, SP"
- comparison: "compare X vs Y"
- top_n: "top 5", "highest"
- trend: "over time", "monthly"

Multiple values default: GROUP BY (show each). Exception: "combined" or "total together" ‚Üí aggregate.

STAGE 2: MAPPING VALIDATION

PRIORITY 0: HISTORY-BASED FILTER RESOLUTION (if history available)
If filter value appears in MULTIPLE columns in EXTRACTED FILTERS:
- Check which column HISTORY SQL used for similar filter
- Verify that column exists in AVAILABLE METADATA
- If both met ‚Üí Use history's column (HIGH CONFIDENCE)
- If not ‚Üí Continue to STEP 2A-2F

‚ö†Ô∏è TIME FILTER EXCEPTION: Do NOT use history for year/month/quarter. Always use current question's time.

STEP 2A: TERM ‚Üí METADATA COLUMN (Semantic OK)
- HIGH CONFIDENCE: ONE column semantically matches
- AMBIGUOUS: Multiple columns match ‚Üí follow-up
- NO MATCH: No related column ‚Üí follow-up

STEP 2B: VALUE ‚Üí SYNONYM MAPPING (CHECK FIRST - HIGH PRIORITY)
‚ö†Ô∏è BEFORE checking extracted filters or metadata samples, check AVAILABLE METADATA for synonym mappings:
- Look for patterns like "Mail->Home Delivery", "SP->Specialty", "HDP->Home Delivery", "PBM Retail->PBM"
- These mappings appear in column descriptions (e.g., "**product_category**: ... Mail->Home Delivery ...")
- IF synonym mapping found ‚Üí Replace value with mapped value ‚Üí Use mapped column (HIGH CONFIDENCE)
- IF no synonym mapping ‚Üí Continue to STEP 2C

Example: Question has "Mail" ‚Üí Metadata shows "Mail->Home Delivery" ‚Üí Use product_category='Home Delivery'

STEP 2C: VALUE ‚Üí EXTRACTED FILTERS (EXACT only)
- FOUND SINGLE COLUMN: Use it ‚úì
- FOUND MULTIPLE COLUMNS: Check PRIORITY 0 first, else follow-up
- NOT FOUND: Continue to 2D

STEP 2D: VALUE ‚Üí METADATA SAMPLES (EXACT only)
- FOUND: Use that column ‚úì
- NOT FOUND: Continue to 2E (if date) or mark UNKNOWN

STEP 2E: DATE/TIME PARSING
Valid formats:
- "July 2025" ‚Üí month=7, year=2025
- "Q3 2024" ‚Üí quarter='Q3', year=2024
- "Q3" alone ‚Üí quarter='Q3'
- "2025" ‚Üí year=2025
- "Jan to March 2025" ‚Üí month IN (1,2,3), year=2025

Months: Jan=1, Feb=2, Mar=3, Apr=4, May=5, Jun=6, Jul=7, Aug=8, Sep=9, Oct=10, Nov=11, Dec=12
Quarters: Q1=(1,2,3), Q2=(4,5,6), Q3=(7,8,9), Q4=(10,11,12)

Invalid ‚Üí follow-up: "recently", "lately", "a while ago"
No time mentioned ‚Üí Do NOT add time filters

STEP 2F: ATTRIBUTE ‚Üí COLUMN
- "by carrier" ‚Üí carrier columns
- "by client" ‚Üí ora_client_id, ora_client_description  
- "by product" ‚Üí product_category, product_sub_category
- "by LOB" ‚Üí line_of_business
Unclear ‚Üí follow-up

STEP 2G: UNKNOWN VALUE CHECK
Each value must pass ONE: Synonym mapped (2B) | Found in EXTRACTED (2C) | Found in METADATA (2D) | Parsed as date (2E) | Is number
ANY value fails all ‚Üí UNKNOWN ‚Üí FOLLOWUP_REQUIRED

STAGE 3: MULTI-TABLE HANDLING

IF single table selected:
- query_type = "SINGLE_TABLE"

IF multiple tables selected:

CHECK 1: Is JOIN INFORMATION provided?
- YES ‚Üí query_type = "MULTI_TABLE_JOIN", use join_info

CHECK 2: Can question be answered by single table?
- YES ‚Üí query_type = "SINGLE_TABLE", use that table only

CHECK 3: Do tables have complementary data (no join possible)?
- YES ‚Üí query_type = "MULTI_TABLE_SEPARATE"
  - Split question into parts
  - Assign each part to appropriate table
  - Each table answers partial question
  - Add "answers_part" to each table indicating what it answers

CHECK 4: None of above?
- Ask follow-up about which table to use

STAGE 4: DECISION

FOLLOWUP_REQUIRED if ANY: Unknown value | Ambiguous column | Ambiguous metric | Unclear attribute | Vague time
SQL_READY only if ALL: All mappings HIGH CONFIDENCE | All values found | No ambiguity

OUTPUT FORMAT:

Output EXACTLY:
1. <reasoning> block
2. Either <followup> OR <context>JSON</context>

<reasoning>
STEP 1 - EXTRACTED:
- Terms: [list]
- Values: [list]  
- Intent: [type]
- User hints: [list or none]

STEP 2 - TERM MAPPINGS:
- "[term]" ‚Üí [column] (HIGH CONFIDENCE | AMBIGUOUS | NO MATCH)

STEP 3 - VALUE MAPPINGS:
- "[value]" ‚Üí SYNONYM: [FOUND mapping‚Üíactual_value | NOT FOUND] ‚Üí EXTRACTED: [FOUND in X | NOT FOUND] ‚Üí METADATA: [FOUND in X | NOT FOUND] ‚Üí Date: [PARSED | N/A] ‚Üí RESULT: [column=value | UNKNOWN]

STEP 4 - MULTI-TABLE CHECK:
- Tables selected: [count]
- Join available: [YES/NO]
- Query type decision: [SINGLE_TABLE | MULTI_TABLE_JOIN | MULTI_TABLE_SEPARATE]
- Reason: [why this decision]

STEP 5 - DECISION: [SQL_READY | FOLLOWUP_REQUIRED]
- Reason: [why]
</reasoning>

IF FOLLOWUP_REQUIRED:

<followup>
I need one clarification to generate accurate SQL:

[AMBIGUITY_TYPE]: [Direct question]

Available options:
1. [column_name] - [description with sample values]
2. [column_name] - [description with sample values]

Please specify which one.
</followup>

Ambiguity types: Unknown filter value | Ambiguous column | Ambiguous metric | Unclear attribute | Vague time reference

IF SQL_READY:

Output valid JSON wrapped in <context> tags. The JSON must follow this exact structure:

<context>
{{
  "decision": "SQL_READY",
  "query_type": "[SINGLE_TABLE | MULTI_TABLE_JOIN | MULTI_TABLE_SEPARATE]",
  
  "tables": [
    {{
      "name": "[full.table.name]",
      "alias": "[alias]",
      "role": "[PRIMARY | SECONDARY]",
      "answers_part": "[what this table answers - only for MULTI_TABLE_SEPARATE, else null]",
      "filters": [
        {{"column": "[col]", "operator": "[=|IN|>|<]", "value": "[val]", "source": "[MANDATORY|QUESTION|EXTRACTED]"}}
      ],
      "columns_needed": ["[col1]", "[col2]"]
    }}
  ],
  
  "join_info": {{
    "type": "[LEFT JOIN | INNER JOIN | null if not applicable]",
    "condition": "[t1.col = t2.col | null]",
    "source": "[PROVIDED | null]"
  }},
  
  "separate_queries_reason": "[reason for MULTI_TABLE_SEPARATE | null if not applicable]",
  
  "metric": {{
    "is_split": [true if MULTI_TABLE_SEPARATE | false],
    "requested_term": "[user term - for single metric]",
    "column": "[column_name - for single metric]",
    "metric_type_filter": "[value or null - for single metric]",
    "additional_metric_types": [],
    "aggregation": "[SUM|COUNT|AVG - for single metric]",
    "parts": [
      {{
        "table": "[alias]",
        "term": "[term]",
        "column": "[col]",
        "metric_type_filter": "[val or null]",
        "aggregation": "[SUM|COUNT]"
      }}
    ]
  }},
  
  "grouping": {{
    "columns": [{{"table": "[alias]", "column": "[col]"}}],
    "intent": "[simple_aggregate|breakdown|comparison|top_n|trend]"
  }},
  
  "time_filters": {{
    "has_time_filter": [true|false],
    "year": [value or null],
    "month": [value or null],
    "quarter": "[value or null]"
  }},
  
  "user_hints_applied": []
}}
</context>

JSON FIELD RULES:
- query_type: Always required
- tables: Array with 1 entry for SINGLE_TABLE, 2+ for multi-table
- tables[].answers_part: Only populate for MULTI_TABLE_SEPARATE, else null
- join_info: Populate for MULTI_TABLE_JOIN, null for others
- separate_queries_reason: Only populate for MULTI_TABLE_SEPARATE, else null
- metric.is_split: true only for MULTI_TABLE_SEPARATE
- metric.parts: Only populate for MULTI_TABLE_SEPARATE, else empty array []
- metric.requested_term/column/metric_type_filter/aggregation: Populate for SINGLE_TABLE and MULTI_TABLE_JOIN

CRITICAL: Your response must contain ONLY the <reasoning> block followed by EITHER <followup> OR <context> with valid JSON inside. Do not include any other text.

"""
        return prompt
    
    def _build_sql_writer_prompt(self, context_json: Dict, state: Dict, current_question: str) -> str:
        """Build the SQL generation prompt with history pattern learning (Call 2)
        
        This prompt handles:
        - History pattern detection (GROUPING SETS, UNION, SIMPLE)
        - Enhancement decision (inherit breakdown from history)
        - metric_type handling (CASE WHEN pivot vs GROUP BY)
        - Component display rule
        - Query patterns (TOP N, TIME COMPARISON, etc.)
        - All SQL formatting rules
        
        Args:
            context_json: Validated context from Call 1
            state: Agent state with history and metadata
            current_question: The current user question from context
        """
        
        history_question_match = state.get('history_question_match', '')
        matched_sql = state.get('matched_sql', '')
        has_history = bool(matched_sql and history_question_match)
        
        # Build history section for pattern learning
        if has_history:
            history_section = f"""

HISTORICAL SQL FOR PATTERN LEARNING

PREVIOUS QUESTION: {history_question_match}

<historical_sql>
{matched_sql}
</historical_sql>

PURPOSE: History represents LEARNED DETAIL PREFERENCES. Your goal is to ENHANCE simple questions with historical detail patterns - not just copy SQL.
PRINCIPLE: If history shows breakdown + totals, provide that detail level even if user asks a simple question.


STAGE 1: PATTERN ANALYSIS (do this FIRST before any SQL)

Analyze HISTORICAL SQL structure:

DETECT PATTERN TYPE:
- Contains "GROUPING SETS" + "GROUPING(" function ‚Üí GROUPING_SETS_TOTAL
- Contains "UNION ALL" + 'Total'/'OVERALL' literal ‚Üí UNION_TOTAL
- Neither ‚Üí SIMPLE

IF GROUPING_SETS_TOTAL detected, extract:
- breakdown_column: column inside GROUPING() function
- parent_dimension: the parent filter column (e.g., product_category)
- grouping_sets_structure: the exact GROUPING SETS clause
- total_label: label used (OVERALL_TOTAL, Total, etc.)
- order_position: total first (0) or last (1) in ORDER BY

IF UNION_TOTAL detected, extract:
- How the total row is constructed
- What literal is used for the total label

STAGE 2: ENHANCEMENT DECISION

ENHANCE with history pattern = YES when ALL true:
‚úì Pattern is GROUPING_SETS_TOTAL or UNION_TOTAL
‚úì Same/similar metric (both ask for revenue, both ask for cost, etc.)
‚úì Current question filters on PARENT dimension of history's breakdown
  (e.g., history breaks down product_sub_category_lvl_2, current filters product_category)
‚úì User did NOT say "total only", "just sum", "single number", "aggregate only"

ENHANCE = NO when ANY true:
‚úó Pattern is SIMPLE (nothing to inherit)
‚úó Different metric type entirely
‚úó User explicitly wants only aggregate total
‚úó Current already has different explicit grouping

WHEN TO APPLY PATTERN (if ENHANCE = YES):
- User explicitly asks for breakdown ‚Üí Apply
- History has breakdown on sub-dimension + current filters parent + same metric ‚Üí Apply
- User asks comparison across time periods ‚Üí Apply

ONLY SKIP PATTERN WHEN:
- User explicitly says "total only", "just the sum", "single number"
"""
        else:
            history_section = """
HISTORICAL SQL:
No historical SQL available. Generate fresh SQL based on context.
Set pattern_detected = NONE and history_sql_used = false
"""


        prompt = f"""You are a Databricks SQL generator for DANA. Generate SQL using validated context and learn patterns from historical SQL.

CURRENT QUESTION: {current_question}

VALIDATED CONTEXT:
{context_json}

{history_section}

STAGE 1: PATTERN ANALYSIS (if history available)

Analyze HISTORICAL SQL structure:

DETECT PATTERN:
- "GROUPING SETS" + "GROUPING(" ‚Üí GROUPING_SETS_TOTAL
- "UNION ALL" + 'Total'/'OVERALL' ‚Üí UNION_TOTAL
- Neither ‚Üí SIMPLE

IF GROUPING_SETS_TOTAL detected, extract:
- breakdown_column: column inside GROUPING()
- parent_dimension: parent filter column
- total_label: label used (OVERALL_TOTAL, etc.)
- order_position: total first (0) or last (1)

STAGE 2: ENHANCEMENT DECISION

ENHANCE with history pattern = YES when ALL true:
‚úì Pattern is GROUPING_SETS_TOTAL or UNION_TOTAL
‚úì Same/similar metric as history
‚úì Current filters on PARENT dimension of history's breakdown
‚úì User did NOT say "total only" / "just sum"

ENHANCE = NO when ANY true:
‚úó Pattern is SIMPLE
‚úó Different metric type
‚úó User explicitly wants only aggregate

STAGE 3: SQL GENERATION RULES

PRIORITY 0: MANDATORY REQUIREMENTS (violation = query failure)

M1. MANDATORY FILTERS - Must be in WHERE clause
- Check context.tables[].filters where source = "MANDATORY"
- If ledger is MANDATORY ‚Üí WHERE UPPER(ledger) = UPPER('GAAP')
- If product_category='PBM' is MANDATORY ‚Üí WHERE UPPER(product_category) = UPPER('PBM')
- NEVER omit mandatory filters

M2. CASE-INSENSITIVE STRING COMPARISON
- Always use: WHERE UPPER(column) = UPPER('value')
- Never use: WHERE column = 'value'

M3. SAFE DIVISION
- Always use: NULLIF(denominator, 0)
- Never use: bare division that could divide by zero

M4. NUMERIC FORMATTING
- Amounts: ROUND(value, 0) AS column_name
- Percentages: ROUND(value, 3) AS column_pct

PRIORITY 1: METRIC TYPE HANDLING (critical for calculations)

When metric.metric_type_filter is provided:

FOR SINGLE METRIC:
- Filter: WHERE UPPER(metric_type) = UPPER('[value]')
- Aggregate: SUM(amount_or_count) or as specified

FOR CALCULATIONS (margin, ratios, differences) with additional_metric_types:
Pivot metric_type into CASE WHEN columns, do NOT group by metric_type:

CORRECT:
SELECT 
    dimension,
    SUM(CASE WHEN UPPER(metric_type) = UPPER('Revenues') THEN amount_or_count ELSE 0 END) AS revenues,
    SUM(CASE WHEN UPPER(metric_type) = UPPER('COGS Post Reclass') THEN amount_or_count ELSE 0 END) AS cogs,
    ROUND(SUM(CASE WHEN UPPER(metric_type) = UPPER('Revenues') THEN amount_or_count ELSE 0 END) - 
          SUM(CASE WHEN UPPER(metric_type) = UPPER('COGS Post Reclass') THEN amount_or_count ELSE 0 END), 0) AS gross_margin
FROM table
WHERE UPPER(metric_type) IN (UPPER('Revenues'), UPPER('COGS Post Reclass'))
GROUP BY dimension

WRONG (breaks calculations):
GROUP BY dimension, metric_type  -- Creates separate rows, can't calculate across

PRIORITY 2: COMPONENT DISPLAY RULE

For ANY calculated metric, show source components:

Example for "cost per script":
SELECT 
  dimension,
  SUM(total_cost) AS total_cost,
  SUM(script_count) AS script_count,
  ROUND(SUM(total_cost) / NULLIF(SUM(script_count), 0), 2) AS cost_per_script
FROM table
GROUP BY dimension

PRIORITY 3: QUERY PATTERNS

PATTERN - TOP N:
SELECT column, SUM(metric) AS metric
FROM table WHERE [filters]
GROUP BY column
ORDER BY metric DESC
LIMIT N

PATTERN - TIME COMPARISON (side-by-side periods):
SELECT dimension,
       SUM(CASE WHEN month = 7 THEN amount_or_count ELSE 0 END) AS jul_value,
       SUM(CASE WHEN month = 8 THEN amount_or_count ELSE 0 END) AS aug_value
FROM table
WHERE [filters] AND month IN (7, 8)
GROUP BY dimension

PATTERN - PERCENTAGE OF TOTAL:
SELECT column,
       SUM(metric) AS value,
       ROUND(SUM(metric) * 100.0 / (SELECT SUM(metric) FROM table WHERE [same filters]), 3) AS pct
FROM table WHERE [filters]
GROUP BY column

PATTERN - BREAKDOWN BY MULTIPLE VALUES:
SELECT product_category, SUM(amount_or_count) AS value
FROM table
WHERE UPPER(product_category) IN (UPPER('HDP'), UPPER('SP'))
GROUP BY product_category

READING CONTEXT BY QUERY TYPE:

IF query_type = "SINGLE_TABLE":
- Table: context.tables[0].name
- Alias: context.tables[0].alias
- Filters: context.tables[0].filters
- Columns: context.tables[0].columns_needed
- Generate single SQL

IF query_type = "MULTI_TABLE_JOIN":
- PRIMARY table: tables[] where role = "PRIMARY"
- SECONDARY table(s): tables[] where role = "SECONDARY"
- Join: context.join_info.type + context.join_info.condition
- Apply each table's filters with table alias
- Generate single SQL with JOIN

IF query_type = "MULTI_TABLE_SEPARATE":
- Generate SEPARATE SQL for each table
- Each table has "answers_part" indicating what it answers
- Use metric.parts[] to get metric details per table
- Apply each table's filters independently
- Output as <multiple_sql> format
- Each query answers part of the user's question

TIME FILTERS:
- If context.time_filters.has_time_filter = true:
  - Add year = [value] if year is not null
  - Add month = [value] if month is not null
  - Add quarter = '[value]' if quarter is not null

GROUPING:
- Use context.grouping.columns for GROUP BY
- Each entry has {{table, column}} - use table alias in multi-table queries
- Intent guides structure: breakdown ‚Üí GROUP BY, simple_aggregate ‚Üí no grouping on dimensions

STAGE 4: APPLY HISTORY PATTERN (if ENHANCE = YES)

IF GROUPING_SETS_TOTAL:
SELECT
    parent_dimension,
    CASE WHEN GROUPING(breakdown_column) = 1 THEN 'OVERALL_TOTAL' ELSE breakdown_column END AS breakdown_column,
    ROUND(SUM(CASE WHEN UPPER(metric_type) = UPPER('Metric') THEN amount_or_count ELSE 0 END), 0) AS metric_alias
FROM table
WHERE [all context filters]
GROUP BY GROUPING SETS (
    (parent_dimension, breakdown_column),
    (parent_dimension)
)
ORDER BY CASE WHEN breakdown_column = 'OVERALL_TOTAL' THEN 0 ELSE 1 END, breakdown_column

IF UNION_TOTAL:
Detail query UNION ALL total query with 'OVERALL_TOTAL' literal

IF ENHANCE = NO:
Generate straightforward SQL based on context.grouping.intent

OUTPUT FORMAT:

FOR SINGLE_TABLE and MULTI_TABLE_JOIN:

<pattern_analysis>
pattern_detected: GROUPING_SETS_TOTAL | UNION_TOTAL | SIMPLE | NONE
breakdown_column: [column or null]
parent_dimension: [column or null]
enhance_decision: YES | NO
enhance_reason: [brief explanation]
</pattern_analysis>

<sql>
[Complete Databricks SQL]
</sql>

<sql_story>
[2-3 sentences explaining the query in business terms]
</sql_story>

<history_sql_used>true | false</history_sql_used>

FOR MULTI_TABLE_SEPARATE:

<pattern_analysis>
pattern_detected: NONE
enhance_decision: NO
enhance_reason: Multiple separate queries - history pattern not applicable
</pattern_analysis>

<multiple_sql>
<query1_title>[What this query answers - max 8 words]</query1_title>
<query1>
[SQL for table 1]
</query1>
<query2_title>[What this query answers - max 8 words]</query2_title>
<query2>
[SQL for table 2]
</query2>
</multiple_sql>

<sql_story>
[Explain that question required data from multiple tables without join relationship, so separate queries were generated. Describe what each query returns.]
</sql_story>

<history_sql_used>false</history_sql_used>
"""
        return prompt

    async def _assess_and_generate_sql_async(self, context: Dict, state: Dict) -> Dict[str, Any]:
        """SQL generation with two-stage approach: Reasoning + SQL Writing
        
        Stage 1 (Call 1): Reasoning & Context Builder
        - Validates filters against metadata
        - Resolves ambiguities
        - Builds structured context
        - Returns follow-up if needed
        
        Stage 2 (Call 2): SQL Generation with Pattern Learning
        - Analyzes history SQL for patterns
        - Decides enhancement strategy
        - Generates SQL with mandatory rules
        """
        
        current_question = context.get('current_question', '')
        dataset_metadata = context.get('dataset_metadata', '')
        selected_datasets = state.get('selected_dataset', [])
        filter_metadata_results = state.get('filter_metadata_results', [])
        
        
        # STEP 1: Search for historical SQL feedback
        
        print(f"üîç Searching feedback SQL embeddings for selected dataset(s): {selected_datasets}")
        feedback_results = await self.db_client.sp_vector_search_feedback_sql(
            current_question, table_names=selected_datasets
        )
        
        matched_sql = ''
        history_question_match = ''
        matched_table_name = ''
        
        if feedback_results:
            print(f"ü§ñ Analyzing {len(feedback_results)} feedback SQL candidates from selected dataset(s)...")
            feedback_selection_result = await self.db_client._llm_feedback_selection(feedback_results, state)
            
            if feedback_selection_result.get('status') == 'match_found':
                matched_seq_id = feedback_selection_result.get('seq_id')
                
                # Find matching record
                matched_record = None
                for result in feedback_results:
                    if result.get('seq_id') == matched_seq_id:
                        matched_record = result
                        break
                
                if matched_record:
                    history_question_match = matched_record.get('user_question', '')
                    matched_sql = matched_record.get('sql_query', '')
                    matched_table_name = matched_record.get('table_name', '')
                    
                    # Store in state
                    state['history_question_match'] = history_question_match
                    state['matched_sql'] = matched_sql
                    state['matched_table_name'] = matched_table_name
                    
                    print(f"‚úÖ Feedback match found from {matched_table_name}")
                    print(f"   Matched question: {history_question_match[:100]}...")
                else:
                    print(f"‚ö†Ô∏è Matched seq_id {matched_seq_id} not found in results")
            else:
                print(f"‚ÑπÔ∏è No suitable feedback SQL match found (status: {feedback_selection_result.get('status')})")
        else:
            print(f"‚ÑπÔ∏è No feedback SQL embeddings found for selected dataset(s)")
        
        has_history = bool(matched_sql and history_question_match and matched_table_name)
        
        
        # STEP 2: Build mandatory columns text
        
        mandatory_column_mapping = {
            "prd_optumrx_orxfdmprdsa.rag.ledger_actual_vs_forecast": ["Ledger"],
            "prd_optumrx_orxfdmprdsa.rag.pbm_claims": ["product_category='PBM'"]
        }
        
        mandatory_columns_info = []
        if isinstance(selected_datasets, list):
            for dataset in selected_datasets:
                if dataset in mandatory_column_mapping:
                    mandatory_columns = mandatory_column_mapping[dataset]
                    for col in mandatory_columns:
                        mandatory_columns_info.append(f"Table {dataset}: {col} (MANDATORY)")
                else:
                    mandatory_columns_info.append(f"Table {dataset}: Not Applicable")
        
        mandatory_columns_text = "\n".join(mandatory_columns_info) if mandatory_columns_info else "No mandatory columns required"
        state['mandatory_columns_text'] = mandatory_columns_text
        
        
        # CALL 1: Reasoning & Context Builder
        
        print("=" * 70)
        print("üìã CALL 1: Running reasoning and validation...")
        print("=" * 70)
        print("Current Timestamp before Call 1:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        reasoning_prompt = self._build_reasoning_prompt(context, state)
        print(f"üìä Call 1 Prompt Length: {len(reasoning_prompt)} chars")
        
        context_json = None
        
        for attempt in range(self.max_retries):
            try:
                reasoning_response = await self.db_client.call_claude_api_endpoint_async(
                    messages=[{"role": "user", "content": reasoning_prompt}],
                    max_tokens=1500,
                    temperature=0.0,
                    top_p=0.1,
                    system_prompt="You are a SQL planning assistant. Analyze questions, validate mappings against metadata, and output structured JSON context. Output ONLY valid JSON, no explanations, no markdown code blocks."
                )
                
                print("Call 1 Raw Response:", reasoning_response)
                print("Current Timestamp after Call 1:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                
                # Parse response - check for <followup> or <context> tags
                clean_response = reasoning_response.strip()
                
                # Check if this is a followup response first
                followup_match = re.search(r'<followup>(.*?)</followup>', clean_response, re.DOTALL)
                if followup_match:
                    followup_text = followup_match.group(1).strip()
                    print(f"‚ùì Follow-up required: {followup_text[:200]}...")
                    
                    state['is_sql_followup'] = True
                    
                    return {
                        'success': True,
                        'needs_followup': True,
                        'sql_followup_questions': followup_text,
                        'used_history_asset': False,
                        'history_sql_used': False,
                        'reasoning_context': None
                    }
                
                # Try to extract JSON from <context> tags
                context_match = re.search(r'<context>(.*?)</context>', clean_response, re.DOTALL)
                if context_match:
                    clean_response = context_match.group(1).strip()
                    print("üì¶ Extracted JSON from <context> tags")
                else:
                    # Remove markdown code blocks if present
                    if clean_response.startswith("```json"):
                        clean_response = clean_response[7:]
                    elif clean_response.startswith("```"):
                        clean_response = clean_response[3:]
                    if clean_response.endswith("```"):
                        clean_response = clean_response[:-3]
                    clean_response = clean_response.strip()
                
                context_json = json.loads(clean_response)
                
                print(f"‚úÖ Call 1 Parsed Successfully: decision={context_json.get('decision')}")
                
                # Legacy check for decision field (in case LLM returns JSON with decision=FOLLOWUP_REQUIRED)
                if context_json.get('decision') == 'FOLLOWUP_REQUIRED':
                    followup_question = context_json.get('followup_question', 'Could you please clarify your question?')
                    state['is_sql_followup'] = True
                    state['reasoning_context'] = context_json
                    
                    print(f"‚ùì Follow-up required (from JSON): {followup_question}")
                    
                    return {
                        'success': True,
                        'needs_followup': True,
                        'sql_followup_questions': followup_question,
                        'used_history_asset': False,
                        'history_sql_used': False,
                        'reasoning_context': context_json
                    }
                
                # Store context for debugging
                state['reasoning_context'] = context_json
                break
                
            except json.JSONDecodeError as e:
                print(f"‚ùå Call 1 JSON parse error (attempt {attempt + 1}): {str(e)}")
                print(f"   Raw response: {reasoning_response[:500]}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {
                        'success': False,
                        'error': f"Reasoning stage failed: Invalid JSON response - {str(e)}",
                        'used_history_asset': False,
                        'history_sql_used': False
                    }
            except Exception as e:
                print(f"‚ùå Call 1 error (attempt {attempt + 1}): {str(e)}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {
                        'success': False,
                        'error': f"Reasoning stage failed: {str(e)}",
                        'used_history_asset': False,
                        'history_sql_used': False
                    }
        
        
        # CALL 2: SQL Generation with History Pattern Learning
        
        print("=" * 70)
        print("üî® CALL 2: Running SQL generation with pattern learning...")
        print("=" * 70)
        print("Current Timestamp before Call 2:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        # Get current_question from context
        current_question = context.get('current_question', '')
        
        sql_writer_prompt = self._build_sql_writer_prompt(context_json, state, current_question)
        print(f"üìä Call 2 Prompt Length: {len(sql_writer_prompt)} chars")
        
        for attempt in range(self.max_retries):
            try:
                sql_response = await self.db_client.call_claude_api_endpoint_async(
                    messages=[{"role": "user", "content": sql_writer_prompt}],
                    max_tokens=2500,
                    temperature=0.0,
                    top_p=0.1,
                    system_prompt="You are a Databricks SQL generator. Analyze historical SQL patterns and generate SQL code. First analyze the pattern, then decide on enhancement, then generate SQL. Output in the specified XML format with pattern_analysis, sql, sql_story, and history_sql_used tags."
                )
                
                print("Call 2 Raw Response:", sql_response[:1500], "..." if len(sql_response) > 1500 else "")
                print("Current Timestamp after Call 2:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                
                # Extract pattern analysis
                pattern_analysis = ""
                pattern_match = re.search(r'<pattern_analysis>(.*?)</pattern_analysis>', sql_response, re.DOTALL)
                if pattern_match:
                    pattern_analysis = pattern_match.group(1).strip()
                    state['pattern_analysis'] = pattern_analysis
                    print(f"üìä Pattern Analysis:\n{pattern_analysis}")
                
                # Extract history_sql_used flag
                history_sql_used = False
                history_used_match = re.search(r'<history_sql_used>\s*(true|false)\s*</history_sql_used>', sql_response, re.IGNORECASE)
                if history_used_match:
                    history_sql_used = history_used_match.group(1).strip().lower() == 'true'
                print(f"üìä History SQL Used: {history_sql_used}")
                
                # Extract sql_story
                sql_story = ""
                story_match = re.search(r'<sql_story>(.*?)</sql_story>', sql_response, re.DOTALL)
                if story_match:
                    sql_story = story_match.group(1).strip()
                    print(f"üìñ SQL Story: {sql_story[:200]}...")
                
                # Extract SQL - check for multiple SQL first
                multiple_sql_match = re.search(r'<multiple_sql>(.*?)</multiple_sql>', sql_response, re.DOTALL)
                if multiple_sql_match:
                    multiple_content = multiple_sql_match.group(1).strip()
                    query_matches = re.findall(
                        r'<query(\d+)_title>(.*?)</query\1_title>.*?<query\1>(.*?)</query\1>',
                        multiple_content, re.DOTALL
                    )
                    
                    if query_matches:
                        sql_queries = []
                        query_titles = []
                        for query_num, title, query in query_matches:
                            cleaned_query = query.strip().replace('`', '')
                            cleaned_title = title.strip()
                            if cleaned_query and cleaned_title:
                                sql_queries.append(cleaned_query)
                                query_titles.append(cleaned_title)
                        
                        if sql_queries:
                            print(f"‚úÖ Generated {len(sql_queries)} SQL queries")
                            return {
                                'success': True,
                                'multiple_sql': True,
                                'sql_queries': sql_queries,
                                'query_titles': query_titles,
                                'query_count': len(sql_queries),
                                'used_history_asset': has_history,
                                'history_sql_used': history_sql_used,
                                'sql_story': sql_story,
                                'pattern_analysis': pattern_analysis,
                                'reasoning_context': context_json
                            }
                
                # Extract single SQL
                sql_match = re.search(r'<sql>(.*?)</sql>', sql_response, re.DOTALL)
                if sql_match:
                    sql_query = sql_match.group(1).strip()
                    # Remove backticks and clean up
                    sql_query = sql_query.replace('`', '')
                    # Remove ```sql and ``` if present
                    if sql_query.startswith('sql'):
                        sql_query = sql_query[3:].strip()
                    
                    if not sql_query:
                        raise ValueError("Empty SQL query in response")
                    
                    print(f"‚úÖ Generated SQL ({len(sql_query)} chars)")
                    print(f"SQL Preview:\n{sql_query[:500]}...")
                    
                    return {
                        'success': True,
                        'multiple_sql': False,
                        'sql_query': sql_query,
                        'used_history_asset': has_history,
                        'history_sql_used': history_sql_used,
                        'sql_story': sql_story,
                        'pattern_analysis': pattern_analysis,
                        'reasoning_context': context_json
                    }
                
                raise ValueError("No SQL found in response")
                
            except Exception as e:
                print(f"‚ùå Call 2 error (attempt {attempt + 1}): {str(e)}")
                if attempt < self.max_retries - 1:
                    print(f"üîÑ Retrying... (attempt {attempt + 2}/{self.max_retries})")
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {
                        'success': False,
                        'error': f"SQL generation failed after {self.max_retries} attempts: {str(e)}",
                        'used_history_asset': has_history,
                        'history_sql_used': False,
                        'reasoning_context': context_json
                    }
        
        return {
            'success': False,
            'error': "SQL generation failed after all retries",
            'used_history_asset': False,
            'history_sql_used': False
        }
