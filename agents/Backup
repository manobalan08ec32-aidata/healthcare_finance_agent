"""
DANA SQL Generator - Two-Stage Approach (Final Version)
Stage 1: SQL Planner - Validates, maps, and outputs unified context
Stage 2: SQL Writer - Takes context directly and generates SQL

Key Design:
- Single unified output format from Planner
- Raw output passes directly to Writer
- No intermediate parsing or format checking needed
"""

import re
import asyncio
from datetime import datetime
from typing import Dict, Any


class DANASQLGenerator:
    """SQL Generator with two-stage reasoning approach"""
    
    def __init__(self, db_client, max_retries: int = 3):
        self.db_client = db_client
        self.max_retries = max_retries

    def _build_sql_planner_prompt(self, context: Dict, state: Dict) -> str:
        """Build the SQL planner prompt (Call 1)
        
        Outputs unified structured context for SQL writer.
        """
        
        current_question = context.get('current_question', '')
        dataset_metadata = context.get('dataset_metadata', '')
        filter_metadata_results = state.get('filter_metadata_results', [])
        mandatory_columns_text = state.get('mandatory_columns_text', '')
        history_question_match = state.get('history_question_match', '')
        matched_sql = state.get('matched_sql', '')

        # History hint for filter resolution
        history_hint = ""
        if matched_sql and history_question_match:
            history_hint = f"""
HISTORY REFERENCE (for filter column resolution - NOT for time filters):
Previous question: {history_question_match}
<historical_sql>
{matched_sql}
</historical_sql>
Use history to validate filter column choices. Never use history for time values.
"""

        prompt = f"""You are DANA's SQL planner. VALIDATE and MAP - never GUESS or ASSUME.

CORE RULES

1. ONE FOLLOW-UP OPPORTUNITY
   You have exactly ONE chance to ask clarification:
   - Unknown value that can't be mapped ‚Üí ASK
   - Multiple columns match same value ‚Üí ASK  
   - Vague time reference ("recently", "lately") ‚Üí ASK
   - Unclear metric or grouping intent ‚Üí ASK
   BETTER TO ASK than to ASSUME WRONG.

2. MAPPING PRINCIPLES
   - TERMS (revenue, cost, count, margin) ‚Üí Match semantically to columns
   - VALUES (MPDOVA, Specialty, HDP) ‚Üí EXACT match only from EXTRACTED FILTERS or METADATA

3. ZERO INVENTION
   - Never add filters not mentioned in question
   - Never assume time period if not specified
   - Never guess column when multiple options exist

INPUTS

QUESTION: {current_question}
METADATA: {dataset_metadata}
MANDATORY FILTERS: {mandatory_columns_text}
EXTRACTED FILTER VALUES: {filter_metadata_results}
{history_hint}

VALIDATION STEPS

„ÄêSTEP 1: PARSE QUESTION„Äë

Extract from question:
- TERMS: Business concepts (revenue, cost, scripts, margin, carrier, client, product)
- VALUES: Specific data points (MPDOVA, Specialty, HDP, July 2025, Q3)
- INTENT: simple_aggregate | breakdown | comparison | top_n | trend
- USER HINTS: Explicit guidance like "use carrier_id" ‚Üí Apply as override

„ÄêSTEP 2: MAP TERMS TO COLUMNS„Äë

For each TERM, find matching column in METADATA:
- Single match found ‚Üí Use it
- Multiple matches ‚Üí Follow-up required
- No match ‚Üí Follow-up required

„ÄêSTEP 2B: BUILD METRIC EXPRESSIONS„Äë

Construct full SQL expression based on METADATA structure:

Pattern A - Direct Column:
  Table has revenue_amount column
  ‚Üí SUM(t1.revenue_amount) AS total_revenue

Pattern B - Metric Type Filter:
  Table has amount + metric_type column
  ‚Üí SUM(CASE WHEN UPPER(t1.metric_type) = UPPER('REVENUE') THEN t1.amount ELSE 0 END) AS total_revenue

Pattern C - Calculated Metric:
  margin = revenue - cost
  ‚Üí SUM(CASE WHEN UPPER(t1.metric_type) = UPPER('REVENUE') THEN t1.amount ELSE 0 END) - SUM(CASE WHEN UPPER(t1.metric_type) = UPPER('COST') THEN t1.amount ELSE 0 END) AS margin

„ÄêSTEP 3: MAP VALUES TO COLUMNS„Äë

For each VALUE, resolve using this priority:

A. SYNONYM CHECK - Look for patterns in METADATA like "Mail‚ÜíHome Delivery", "SP‚ÜíSpecialty"
   If found ‚Üí Use mapped column with mapped value

B. EXTRACTED FILTERS CHECK - Search for exact value match:
   - Value in ONE column ‚Üí Use it
   - Value in MULTIPLE columns ‚Üí Check HISTORY, if no history ‚Üí follow-up
   - Value NOT found ‚Üí Continue to METADATA check

C. HISTORY SQL REFERENCE (if available)
   - Value in MULTIPLE columns + History used one ‚Üí Use history's column
   - Value in SINGLE column + Same in history ‚Üí Confirms mapping
   ‚ö†Ô∏è Never use history for TIME filters

D. METADATA SAMPLES CHECK - Search sample values in column descriptions
   Found ‚Üí Use that column

E. VALUE NOT MAPPED - If fails all checks and not a number ‚Üí Follow-up required

„ÄêSTEP 4: MAP TIME FILTERS„Äë

If question contains time references:
1. PARSE naturally (July 2025, Q3 2024, 2025, Jan to March 2025, YTD)
   Vague like "recently", "lately" ‚Üí Follow-up required
2. MAP to date columns in METADATA (year, month, quarter, or date columns)
3. CONSTRUCT filter with correct data type

No time mentioned ‚Üí Do NOT add time filters

„ÄêSTEP 5: MANDATORY FILTER CHECK„Äë

Every MANDATORY filter must appear in output.
Missing mandatory ‚Üí Cannot generate SQL

„ÄêSTEP 6: MULTI-TABLE HANDLING„Äë

Single table ‚Üí Include one QUERY block
Multiple tables with JOIN INFO ‚Üí Include JOIN details
Multiple tables, no join ‚Üí Include separate QUERY blocks

„ÄêSTEP 7: FINAL DECISION„Äë

FOLLOWUP_REQUIRED if ANY: Unknown value | Ambiguous column | Vague time | Unclear intent
SQL_READY if ALL: Every term mapped | Every value mapped | Time mapped (or none needed)

OUTPUT FORMAT

Output ONLY <context> block, optionally followed by <followup>:

<context>
DECISION: [SQL_READY | FOLLOWUP_REQUIRED]
QUERY_TYPE: [SINGLE_TABLE | MULTI_TABLE_JOIN | MULTI_TABLE_SEPARATE]
INTENT: [simple_aggregate | breakdown | comparison | top_n | trend]

QUERY_1:
TABLE: [full.table.name] AS [alias]
ANSWERS: [what this query answers - use "full question" for single query]

SELECT:
- [t1.column1]
- [SUM(CASE WHEN UPPER(t1.metric_type) = UPPER('REVENUE') THEN t1.amount ELSE 0 END) AS revenue]

FILTERS:
- [UPPER(t1.carrier_id) = UPPER('MPDOVA')] [STRING]
- [t1.year = 2025] [INT]
- [t1.month = 7] [INT]
- [UPPER(t1.ledger) = UPPER('GAAP')] [MANDATORY]

GROUP_BY: [t1.column1, t1.column2] or [none]
ORDER_BY: [revenue DESC] or [none]
LIMIT: [10] or [none]

JOIN: [t1.key = t2.key LEFT JOIN] or [none]

QUERY_2 (only if MULTI_TABLE_SEPARATE or MULTI_TABLE_JOIN):
TABLE: [full.table.name] AS [alias]
ANSWERS: [what this query answers]

SELECT:
- [columns and expressions]

FILTERS:
- [filters with type tags]

GROUP_BY: [columns] or [none]
ORDER_BY: [direction] or [none]
LIMIT: [number] or [none]
</context>

IF FOLLOWUP_REQUIRED, add after </context>:

<followup>
I need one clarification to generate accurate SQL:

[Brief question about the specific ambiguity]

Options:
1. [column_name] - [brief description with sample values]
2. [column_name] - [brief description with sample values]

Which one did you mean?
</followup>

RULES FOR OUTPUT
- Always include DECISION, QUERY_TYPE, INTENT at top
- Always use QUERY_1 block (even for single table)
- QUERY_2 only when multiple tables needed
- FILTERS must include data type: [STRING], [INT], [DATE]
- FILTERS must mark [MANDATORY] for mandatory filters
- String filters must use UPPER(): UPPER(col) = UPPER('value')
- SELECT expressions must be complete and ready to use
- Use table alias (t1, t2) for all column references
"""
        return prompt


    def _build_sql_writer_prompt(self, context_output: str, state: Dict, current_question: str) -> str:
        """Build the SQL writer prompt (Call 2)
        
        Takes raw context from planner and generates SQL with pattern learning.
        """
        
        history_question_match = state.get('history_question_match', '')
        matched_sql = state.get('matched_sql', '')
        has_history = bool(matched_sql and history_question_match)
        
        # Build history section
        if has_history:
            history_section = f"""
HISTORICAL SQL FOR PATTERN LEARNING

PREVIOUS QUESTION: {history_question_match}

<historical_sql>
{matched_sql}
</historical_sql>

PURPOSE: History represents LEARNED DETAIL PREFERENCES. Enhance simple questions with historical detail patterns.
PRINCIPLE: If history shows breakdown + totals, provide that detail level even if user asks simple question.

PATTERN DETECTION:

DETECT PATTERN TYPE:
- Contains "GROUPING SETS" + "GROUPING(" function ‚Üí GROUPING_SETS_TOTAL
- Contains "UNION ALL" + 'Total'/'OVERALL' literal ‚Üí UNION_TOTAL
- Neither ‚Üí SIMPLE

IF GROUPING_SETS_TOTAL detected, extract:
- breakdown_column: column inside GROUPING() function
- parent_dimension: the parent filter column
- total_label: label used (OVERALL_TOTAL, Total, etc.)
- order_position: total first (0) or last (1) in ORDER BY

IF UNION_TOTAL detected, extract:
- How the total row is constructed
- What literal is used for the total label

ENHANCEMENT DECISION:

ENHANCE = YES when ALL true:
‚úì Pattern is GROUPING_SETS_TOTAL or UNION_TOTAL
‚úì Same/similar metric (both ask for revenue, both ask for cost, etc.)
‚úì Current question filters on PARENT dimension of history's breakdown
‚úì User did NOT say "total only", "just sum", "single number", "aggregate only"

ENHANCE = NO when ANY true:
‚úó Pattern is SIMPLE (nothing to inherit)
‚úó Different metric type entirely
‚úó User explicitly wants only aggregate total
‚úó Current already has different explicit grouping
"""
        else:
            history_section = """
HISTORICAL SQL:
No historical SQL available. Generate fresh SQL based on context.
Set pattern_detected = NONE and history_sql_used = false
"""

        prompt = f"""You are a Databricks SQL generator for DANA. Generate SQL using planned context and learn patterns from historical SQL.

CURRENT QUESTION: {current_question}

PLANNED CONTEXT:
{context_output}
{history_section}

SQL GENERATION RULES

PRIORITY 0: MANDATORY REQUIREMENTS (violation = query failure)

M1. MANDATORY FILTERS
- Filters marked [MANDATORY] MUST be in WHERE clause
- NEVER omit mandatory filters

M2. CASE-INSENSITIVE STRING COMPARISON
- Context already has UPPER() applied - use expressions as provided
- For any new strings: WHERE UPPER(column) = UPPER('value')

M3. SAFE DIVISION
- Always use: NULLIF(denominator, 0)
- Never use: bare division that could divide by zero

M4. NUMERIC FORMATTING
- Amounts: ROUND(value, 0) AS column_name
- Percentages: ROUND(value, 3) AS column_pct

PRIORITY 1: METRIC TYPE HANDLING (critical for calculations)

FOR SINGLE METRIC with CASE WHEN in SELECT:
- Use the expression exactly as provided in context
- Do NOT add metric_type to GROUP BY

FOR CALCULATIONS (margin, ratios, differences):
Pivot metric_type into CASE WHEN columns, do NOT group by metric_type:

CORRECT:
SELECT 
    dimension,
    SUM(CASE WHEN UPPER(metric_type) = UPPER('Revenues') THEN amount ELSE 0 END) AS revenues,
    SUM(CASE WHEN UPPER(metric_type) = UPPER('COGS') THEN amount ELSE 0 END) AS cogs,
    ROUND(SUM(CASE WHEN UPPER(metric_type) = UPPER('Revenues') THEN amount ELSE 0 END) - 
          SUM(CASE WHEN UPPER(metric_type) = UPPER('COGS') THEN amount ELSE 0 END), 0) AS gross_margin
FROM table
WHERE UPPER(metric_type) IN (UPPER('Revenues'), UPPER('COGS'))
GROUP BY dimension

WRONG (breaks calculations):
GROUP BY dimension, metric_type  -- Creates separate rows, can't calculate across

PRIORITY 2: COMPONENT DISPLAY RULE

For ANY calculated metric, show source components:

Example for "cost per script":
SELECT 
  dimension,
  ROUND(SUM(total_cost), 0) AS total_cost,
  ROUND(SUM(script_count), 0) AS script_count,
  ROUND(SUM(total_cost) / NULLIF(SUM(script_count), 0), 2) AS cost_per_script
FROM table
GROUP BY dimension

PRIORITY 3: QUERY PATTERNS

PATTERN - TOP N:
SELECT column, ROUND(SUM(metric), 0) AS metric
FROM table WHERE [filters]
GROUP BY column
ORDER BY metric DESC
LIMIT N

PATTERN - TIME COMPARISON (side-by-side periods):
SELECT dimension,
       ROUND(SUM(CASE WHEN month = 7 THEN amount ELSE 0 END), 0) AS jul_value,
       ROUND(SUM(CASE WHEN month = 8 THEN amount ELSE 0 END), 0) AS aug_value
FROM table
WHERE [filters] AND month IN (7, 8)
GROUP BY dimension

PATTERN - PERCENTAGE OF TOTAL:
SELECT column,
       ROUND(SUM(metric), 0) AS value,
       ROUND(SUM(metric) * 100.0 / NULLIF((SELECT SUM(metric) FROM table WHERE [same filters]), 0), 3) AS pct
FROM table WHERE [filters]
GROUP BY column

PATTERN - BREAKDOWN BY MULTIPLE VALUES:
SELECT product_category, ROUND(SUM(amount), 0) AS value
FROM table
WHERE UPPER(product_category) IN (UPPER('HDP'), UPPER('SP'))
GROUP BY product_category

READING CONTEXT:

QUERY_TYPE = SINGLE_TABLE:
- Read QUERY_1 block
- Build single SELECT...FROM...WHERE...GROUP BY statement

QUERY_TYPE = MULTI_TABLE_JOIN:
- Read QUERY_1 and QUERY_2 blocks
- Use JOIN clause from context
- Build single SQL with JOIN

QUERY_TYPE = MULTI_TABLE_SEPARATE:
- Read each QUERY_N block
- Generate SEPARATE SQL for each
- Each query answers part of the question (see ANSWERS field)
- Output in <multiple_sql> format

BUILDING SQL FROM CONTEXT:

1. SELECT: Use expressions from SELECT section exactly as provided
2. FROM: Use TABLE from context with alias
3. WHERE: Apply all FILTERS from context (already have UPPER() for strings)
4. GROUP BY: Use GROUP_BY from context (skip if "none")
5. ORDER BY: Use ORDER_BY from context (skip if "none")
6. LIMIT: Use LIMIT from context (skip if "none")

APPLY HISTORY PATTERN (if ENHANCE = YES):

IF GROUPING_SETS_TOTAL:
SELECT
    parent_dimension,
    CASE WHEN GROUPING(breakdown_column) = 1 THEN 'OVERALL_TOTAL' ELSE breakdown_column END AS breakdown_column,
    ROUND(SUM(CASE WHEN UPPER(metric_type) = UPPER('Metric') THEN amount ELSE 0 END), 0) AS metric_alias
FROM table
WHERE [all context filters]
GROUP BY GROUPING SETS (
    (parent_dimension, breakdown_column),
    (parent_dimension)
)
ORDER BY parent_dimension, CASE WHEN breakdown_column = 'OVERALL_TOTAL' THEN 0 ELSE 1 END, breakdown_column

IF UNION_TOTAL:
-- Detail query
SELECT dimension, breakdown_col, ROUND(SUM(metric), 0) AS metric
FROM table WHERE [filters]
GROUP BY dimension, breakdown_col

UNION ALL

-- Total query  
SELECT dimension, 'OVERALL_TOTAL' AS breakdown_col, ROUND(SUM(metric), 0) AS metric
FROM table WHERE [filters]
GROUP BY dimension

ORDER BY dimension, CASE WHEN breakdown_col = 'OVERALL_TOTAL' THEN 0 ELSE 1 END

IF ENHANCE = NO:
Generate straightforward SQL based on context INTENT:
- simple_aggregate ‚Üí No GROUP BY on dimensions, just aggregate
- breakdown ‚Üí GROUP BY dimension columns
- comparison ‚Üí Side-by-side CASE WHEN for periods/categories
- top_n ‚Üí ORDER BY metric DESC LIMIT N
- trend ‚Üí GROUP BY time dimension, ORDER BY time

OUTPUT FORMAT

FOR SINGLE_TABLE and MULTI_TABLE_JOIN:

<pattern_analysis>
pattern_detected: [GROUPING_SETS_TOTAL | UNION_TOTAL | SIMPLE | NONE]
breakdown_column: [column or null]
parent_dimension: [column or null]
enhance_decision: [YES | NO]
enhance_reason: [brief explanation]
</pattern_analysis>

<sql>
[Complete Databricks SQL]
</sql>

<sql_story>
[2-3 sentences explaining the query in business terms]
</sql_story>

<history_sql_used>[true | false]</history_sql_used>

FOR MULTI_TABLE_SEPARATE:

<pattern_analysis>
pattern_detected: NONE
enhance_decision: NO
enhance_reason: Multiple separate queries - history pattern not applicable
</pattern_analysis>

<multiple_sql>
<query1_title>[From QUERY_1 ANSWERS field - max 8 words]</query1_title>
<query1>
[SQL for QUERY_1]
</query1>
<query2_title>[From QUERY_2 ANSWERS field - max 8 words]</query2_title>
<query2>
[SQL for QUERY_2]
</query2>
</multiple_sql>

<sql_story>
[Explain that question required data from multiple tables without join relationship. Describe what each query returns.]
</sql_story>

<history_sql_used>false</history_sql_used>
"""
        return prompt


    def _extract_context_and_followup(self, response: str) -> tuple[str, bool, str]:
        """Extract context block and check for followup
        
        Returns:
            tuple: (context_content, needs_followup, followup_text)
        """
        
        # Extract context block
        context_match = re.search(r'<context>(.*?)</context>', response, re.DOTALL)
        if not context_match:
            raise ValueError("No <context> block found in response")
        
        context_content = context_match.group(1).strip()
        
        # Check for followup - either in tag or in DECISION
        followup_match = re.search(r'<followup>(.*?)</followup>', response, re.DOTALL)
        needs_followup = bool(followup_match) or 'DECISION: FOLLOWUP_REQUIRED' in context_content
        followup_text = followup_match.group(1).strip() if followup_match else ""
        
        return context_content, needs_followup, followup_text


    async def _assess_and_generate_sql_async(self, context: Dict, state: Dict) -> Dict[str, Any]:
        """SQL generation with two-stage approach
        
        Stage 1 (SQL Planner): Validates and builds context
        Stage 2 (SQL Writer): Generates SQL from context
        """
        
        current_question = context.get('current_question', '')
        selected_datasets = state.get('selected_dataset', [])
        
        # ============================================================
        # STEP 1: Search for historical SQL feedback
        # ============================================================
        print(f"üîç Searching feedback SQL for: {selected_datasets}")
        feedback_results = await self.db_client.sp_vector_search_feedback_sql(
            current_question, table_names=selected_datasets
        )
        
        matched_sql = ''
        history_question_match = ''
        matched_table_name = ''
        
        if feedback_results:
            print(f"ü§ñ Analyzing {len(feedback_results)} feedback candidates...")
            feedback_selection_result = await self.db_client._llm_feedback_selection(feedback_results, state)
            
            if feedback_selection_result.get('status') == 'match_found':
                matched_seq_id = feedback_selection_result.get('seq_id')
                
                for result in feedback_results:
                    if result.get('seq_id') == matched_seq_id:
                        history_question_match = result.get('user_question', '')
                        matched_sql = result.get('sql_query', '')
                        matched_table_name = result.get('table_name', '')
                        
                        state['history_question_match'] = history_question_match
                        state['matched_sql'] = matched_sql
                        state['matched_table_name'] = matched_table_name
                        
                        print(f"‚úÖ History match: {matched_table_name}")
                        break
        
        has_history = bool(matched_sql and history_question_match)
        
        # ============================================================
        # STEP 2: Build mandatory columns
        # ============================================================
        mandatory_column_mapping = {
            "prd_optumrx_orxfdmprdsa.rag.ledger_actual_vs_forecast": ["Ledger"],
            "prd_optumrx_orxfdmprdsa.rag.pbm_claims": ["product_category='PBM'"]
        }
        
        mandatory_columns_info = []
        if isinstance(selected_datasets, list):
            for dataset in selected_datasets:
                if dataset in mandatory_column_mapping:
                    for col in mandatory_column_mapping[dataset]:
                        mandatory_columns_info.append(f"Table {dataset}: {col} (MANDATORY)")
        
        state['mandatory_columns_text'] = "\n".join(mandatory_columns_info) if mandatory_columns_info else "None"
        
        # ============================================================
        # CALL 1: SQL Planner
        # ============================================================
        print("=" * 60)
        print("üìã CALL 1: SQL Planner")
        print("=" * 60)
        
        planner_prompt = self._build_sql_planner_prompt(context, state)
        print(f"üìä Prompt: {len(planner_prompt)} chars | {datetime.now().strftime('%H:%M:%S')}")
        
        context_output = None
        
        for attempt in range(self.max_retries):
            try:
                planner_response = await self.db_client.call_claude_api_endpoint_async(
                    messages=[{"role": "user", "content": planner_prompt}],
                    max_tokens=1500,
                    temperature=0.0,
                    top_p=0.1,
                    system_prompt="You are a SQL planner. Output ONLY <context> block, optionally followed by <followup>. No other text."
                )
                
                print(f"Response: {len(planner_response)} chars")
                print(f"Preview: {planner_response[:600]}...")
                
                # Extract context and check followup
                context_output, needs_followup, followup_text = self._extract_context_and_followup(planner_response)
                
                if needs_followup:
                    print(f"‚ùì Followup needed: {followup_text[:150]}...")
                    state['is_sql_followup'] = True
                    state['reasoning_context'] = context_output
                    
                    return {
                        'success': True,
                        'needs_followup': True,
                        'sql_followup_questions': followup_text,
                        'used_history_asset': False,
                        'history_sql_used': False,
                        'reasoning_context': context_output
                    }
                
                print("‚úÖ Planner complete - SQL_READY")
                state['reasoning_context'] = context_output
                break
                
            except Exception as e:
                print(f"‚ùå Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {
                        'success': False,
                        'error': f"SQL Planner failed: {e}",
                        'used_history_asset': False,
                        'history_sql_used': False
                    }
        
        # ============================================================
        # CALL 2: SQL Writer
        # ============================================================
        print("=" * 60)
        print("üî® CALL 2: SQL Writer")
        print("=" * 60)
        
        writer_prompt = self._build_sql_writer_prompt(context_output, state, current_question)
        print(f"üìä Prompt: {len(writer_prompt)} chars | {datetime.now().strftime('%H:%M:%S')}")
        
        for attempt in range(self.max_retries):
            try:
                writer_response = await self.db_client.call_claude_api_endpoint_async(
                    messages=[{"role": "user", "content": writer_prompt}],
                    max_tokens=2500,
                    temperature=0.0,
                    top_p=0.1,
                    system_prompt="You are a Databricks SQL generator. Generate SQL from the provided context. Output in the specified XML format."
                )
                
                print(f"Response: {len(writer_response)} chars")
                print(f"Preview: {writer_response[:800]}...")
                
                # Extract pattern analysis
                pattern_analysis = ""
                pattern_match = re.search(r'<pattern_analysis>(.*?)</pattern_analysis>', writer_response, re.DOTALL)
                if pattern_match:
                    pattern_analysis = pattern_match.group(1).strip()
                    state['pattern_analysis'] = pattern_analysis
                
                # Extract history_sql_used
                history_sql_used = False
                history_match = re.search(r'<history_sql_used>\s*(true|false)\s*</history_sql_used>', writer_response, re.IGNORECASE)
                if history_match:
                    history_sql_used = history_match.group(1).strip().lower() == 'true'
                
                # Extract sql_story
                sql_story = ""
                story_match = re.search(r'<sql_story>(.*?)</sql_story>', writer_response, re.DOTALL)
                if story_match:
                    sql_story = story_match.group(1).strip()
                
                # Check for multiple SQL
                multiple_match = re.search(r'<multiple_sql>(.*?)</multiple_sql>', writer_response, re.DOTALL)
                if multiple_match:
                    multiple_content = multiple_match.group(1).strip()
                    query_matches = re.findall(
                        r'<query(\d+)_title>(.*?)</query\1_title>.*?<query\1>(.*?)</query\1>',
                        multiple_content, re.DOTALL
                    )
                    
                    if query_matches:
                        sql_queries = []
                        query_titles = []
                        for _, title, query in query_matches:
                            cleaned_query = query.strip().replace('`', '')
                            cleaned_title = title.strip()
                            if cleaned_query:
                                sql_queries.append(cleaned_query)
                                query_titles.append(cleaned_title)
                        
                        if sql_queries:
                            print(f"‚úÖ Generated {len(sql_queries)} queries")
                            return {
                                'success': True,
                                'multiple_sql': True,
                                'sql_queries': sql_queries,
                                'query_titles': query_titles,
                                'query_count': len(sql_queries),
                                'used_history_asset': has_history,
                                'history_sql_used': history_sql_used,
                                'sql_story': sql_story,
                                'pattern_analysis': pattern_analysis,
                                'reasoning_context': context_output
                            }
                
                # Extract single SQL
                sql_match = re.search(r'<sql>(.*?)</sql>', writer_response, re.DOTALL)
                if sql_match:
                    sql_query = sql_match.group(1).strip().replace('`', '')
                    if sql_query.startswith('sql'):
                        sql_query = sql_query[3:].strip()
                    
                    if not sql_query:
                        raise ValueError("Empty SQL")
                    
                    print(f"‚úÖ Generated SQL: {len(sql_query)} chars")
                    return {
                        'success': True,
                        'multiple_sql': False,
                        'sql_query': sql_query,
                        'used_history_asset': has_history,
                        'history_sql_used': history_sql_used,
                        'sql_story': sql_story,
                        'pattern_analysis': pattern_analysis,
                        'reasoning_context': context_output
                    }
                
                raise ValueError("No SQL in response")
                
            except Exception as e:
                print(f"‚ùå Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {
                        'success': False,
                        'error': f"SQL Writer failed: {e}",
                        'used_history_asset': has_history,
                        'history_sql_used': False,
                        'reasoning_context': context_output
                    }
        
        return {
            'success': False,
            'error': "SQL generation failed",
            'used_history_asset': False,
            'history_sql_used': False
        }
