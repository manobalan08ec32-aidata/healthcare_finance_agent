selection_prompt = f"""
        You are a meticulous dataset router. Your ONLY job is to find datasets that have the required columns.

        USER QUESTION: "{user_question}"

        AVAILABLE DATASETS:
        {json.dumps(search_results[:5], indent=2)}

        DECISION PROCESS (IN STRICT ORDER):

        1. **IDENTIFY REQUIRED COLUMNS**
        Extract from the user question:
        - Required metrics (revenue, cost, volume, etc.)
        - Required dimensions (drug, pharmacy, month, product_category, etc.)
        - If user says "claim revenue" → needs revenue column (the word "claim" is a hint about source, not a column)
        - If user says "drug analysis" → MUST have drug-related columns

        2. **CHECK COLUMN AVAILABILITY (MOST IMPORTANT)**
        For each dataset:
        - Can it provide ALL required columns? If NO → Cannot use this dataset
        - Can it provide MOST required columns? → Note coverage percentage
        - Special rule: If a required dimension exists in ONLY one dataset → That dataset wins

        3. **APPLY SELECTION RULES**
        
        AUTOMATIC SELECTION (NO FOLLOW-UP):
        - If only ONE dataset has ALL required columns → SELECT IT
        - If one dataset has a UNIQUE required column (e.g., only claims has 'drug') → SELECT IT  
        - If one dataset has >80% coverage and others have <50% → SELECT IT
        
        WHEN TO ASK FOLLOW-UP:
        - Multiple datasets have ALL required columns (>90% coverage each)
        - AND no unique columns differentiate them
        - AND no clear keyword indicates preference
        - Then ask: "Would you like data from [Dataset A description] or [Dataset B description]?"

        4. **TIME GRAIN CHECK**
        - Verify selected dataset supports needed granularity
        - Remember: daily data can aggregate to monthly, but monthly can't go to daily

        5. **USEFULNESS TAGS (ONLY AS TIE-BREAKER)**
        - Only consider these if multiple datasets have identical column coverage
        - 'not_suitable_for' is a hint, not a disqualifier
        
        CRITICAL RULES:
        - NEVER exclude a dataset if it's the ONLY one with required columns
        - Column availability beats everything else
        - "Claim revenue" needs revenue column from claims data (if claims has revenue)
        - Don't overthink - if dataset has the columns, it can answer the question

        RESPONSE FORMAT:
        {{
            "final_actual_tables": ["actual_table_name"],
            "functional_names": ["user-friendly name"],
            "requires_clarification": false,
            "clarification_question": null,
            "candidate_actual_tables": [],
            "selection_reasoning": "Required columns: [list]. Only [dataset] has all required columns including [unique column if applicable]."
        }}

        EXAMPLES OF SELECTION LOGIC:
        
        "Top 10 drugs by revenue":
        - Needs: drug column, revenue column
        - Claims: has drug ✓, revenue ✓ 
        - Ledger: has revenue ✓, NO drug ✗
        - Result: Claims (only dataset with drug column)
        
        "Claim revenue for July 2025 by product category":
        - Needs: revenue, product_category, month
        - Claims: has all ✓
        - Ledger: has all ✓  
        - Keyword "claim" suggests claims dataset
        - Result: Claims (keyword preference when both have columns)
        
        "Revenue by month":
        - Needs: revenue, month
        - Both have these columns
        - No unique column, no clear keyword
        - Result: Ask follow-up
        """





selection_prompt = f"""
                    You are a meticulous dataset router. Choose EXACTLY ONE dataset.

                    USER QUESTION: "{user_question}"

                    DATASETS (JSON array). Each dataset has:
                    - name,description,metrics,attributes,columns,hints,time_grains

                    DATA:
                    {json.dumps(dataset_options, indent=2)}

                    GOAL
                    Map the user question to required columns using ONLY the dataset metadata and order of meta data is random. Evaluate BOTH datasets. Prefer a table that can satisfy ALL required columns and the requested time grain. If no table can fully satisfy, return the closest table by coverage.

                    Follow this decision process strictly:

                    1. **Match Attributes and Metrics First**
                    - Check if the dataset contains the attributes and metrics mentioned or implied in the user question.
                    - Prioritize exact matches (e.g., 'line_of_business', 'month', 'revenue').

                    2. **Check Time Granularity**
                    - Ensure the dataset supports the required time grain (e.g., monthly, daily).

                    3. **Evaluate Usefulness Tags**
                    - If the dataset is marked as 'useful_for' the type of analysis requested, that increases its relevance.
                    - If the dataset is marked as 'not_useful_for' the type of analysis requested, it should be excluded.

                    4. **Select Only One Dataset**
                    - Choose the single best dataset that satisfies the above criteria.

                    RESPONSE FORMAT:
                    The response MUST be valid JSON. Do NOT include any extra text, markdown, or formatting. The response MUST not start with ```json and end with ```.

                    {{ "clear_selection": true, "selected_dataset": "<one of the dataset 'table_name' values>", "selection_reasoning": "One concise sentence referencing why the dataset is best match (e.g., therapy_class_name + month required; only claims has therapy_class_name and supports month/day)." }}

                    """


selection_prompt = f"""
        You are a decisive dataset router. Follow this decision process strictly:

        USER QUESTION: "{user_question}"

        AVAILABLE DATASETS:
        {json.dumps(search_results[:5], indent=2)}

        DECISION PROCESS:
        1. **Match Attributes and Metrics First**
        - Check if the dataset contains the attributes and metrics mentioned or implied in the user question
        - Prioritize exact matches (e.g., 'line_of_business', 'month', 'revenue')
        
        2. **Check Time Granularity**
        - Ensure the dataset supports the required time grain (e.g., monthly, daily)
        
        3. **Evaluate Usefulness Tags**
        - If dataset is marked as 'useful_for' the type of analysis requested, increase relevance
        - If dataset is marked as 'not_useful_for' the type of analysis requested, exclude it
        
        4. **Multi-Table Analysis Detection**
        - Detect if query requires joining data across tables (e.g., claim amounts from different tables)
        - Identify queries that benefit from complementary data perspectives (volumes + financials)
        - Recognize queries needing separate analysis on related datasets (demographics + transactions)
        
        5. **Dataset Selection Strategy**
        - Choose the single best dataset that satisfies the above criteria
        - Select multiple datasets only when analysis inherently requires multiple tables
        
        WHEN TO ASK FOLLOW-UP (RARE CASES ONLY):
        
        **Scenario 1: Data Type Ambiguity**
        - ONLY when both tables have same LOB and user didn't specify "ledger" vs "claims" keywords
        - Example: "Which dataset: claims transactions or financial ledger data?"
        
        **Scenario 2: Multi-Table Analysis Confirmation**
        - When query requires joining data across tables (e.g., billed amount from Table A + paid amount from Table B)
        - When query benefits from complementary perspectives (transaction volumes + financial metrics)
        - When query needs separate analysis on related datasets then combining insights
        - Example: "This analysis requires both Claims Details and Payment Records tables. Proceed with both?"
        
        DO NOT ASK FOLLOW-UP FOR GENERAL AMBIGUITIES:
        - Time period vagueness (e.g., "show pharmacy sales" without specifying when) → Pick most recent complete period
        - Aggregation level uncertainty (e.g., "revenue data" without daily/monthly) → Use most appropriate grain available
        - Scope ambiguity (e.g., "claim trends" without LOB specified) → Use broadest/most complete dataset
        - Metric preference vagueness (e.g., "pharmacy performance" unclear on volume vs financial) → Pick best match based on context
        - BE DECISIVE: Make reasonable choices for these ambiguities rather than asking clarification
        
        MULTI-TABLE EXAMPLES:
        - "What is the claim paid and billed amount?" → Need claims table + payments table (JOIN required)
        - "Show pharmacy performance trends" → Transaction data + Financial metrics (Complementary analysis)
        - "Compare member demographics with claim patterns" → Demographics table + Claims table (Separate analysis)
        
        RESPONSE FORMAT (valid JSON only, no markdown):
        {{
            "final_actual_tables": ["actual_table_name1"] or ["table1", "table2"] if multiple needed,
            "functional_names": ["user-friendly name 1"] or ["name1", "name2"] if multiple,
            "requires_clarification": false,
            "clarification_question": null,
            "candidate_actual_tables": [],
            "selection_reasoning": "Brief explanation of selection based on attribute/metric match"
        }}
        
        IMPORTANT: 
        - When LLM decides on right dataset(s), ALWAYS set requires_clarification: false and clarification_question: null
        - Only set requires_clarification: true and populate clarification_question for the 2 scenarios mentioned above
        - Keep clarification_question short and direct when needed
        - candidate_actual_tables should be populated only when requires_clarification: true
        """

llm response router {
    "final_actual_tables": ["prd_optumrx_orxfdmprdsa.rag.actuals_vs_forecast_analysis"],
    "functional_names": ["Actuals vs Forecast Analysis"],
    "requires_clarification": false,
    "clarification_question": null,
    "candidate_actual_tables": [],
    "selection_reasoning": "Query asks for claim revenue by month and product category PBM. The actuals_vs_forecast_analysis table has exact matches for 'revenue', 'month', and 'product category' dimensions with monthly temporal granularity. The claim_transaction table, while having revenue data, is designed for claim-level analysis and marked as 'not suitable for ledger-level summaries', whereas this query appears to need aggregated revenue metrics by time period and product category."
}

      [
  {
    "llm_context": "{\"table_name\":\"prd_optumrx_orxfdmprdsa.rag.claim_transaction_for_pharmacy_pbm\",\"purpose\":\"Claim-level pharmacy transactions with detailed drug, client, and pharmacy attributes. Individual claim records with payment status for utilization and financial analysis.\",\"core_capabilities\":\"revenue per script analysis, drug utilization trending, therapy class performance (GLP-1, SGLT-2, Oncology), generic dispense rate (GDR), pharmacy network analysis, client-level metrics, brand vs generic mix, claim status tracking\",\"key_measures\":[\"revenue\",\"expense\",\"WAC\",\"AWP\",\"unadjusted scripts\",\"adjusted scripts\",\"30-day scripts\",\"90-day scripts\",\"revenue per script\",\"volume\",\"Generic dispensing ratio/GDR\"],\"key_dimensions\":[\"claim identifiers\",\"claim status\",\"client id/Client name/Client type\",\"carrier/account/group/CAG\",\"pharmacy name/NPI/Pharmacy type\",\"drug name/ NDC Code\",\"therapy class\",\"GPI\",\"line of business\",\"brand vs generic\",\"product category\",\"state code\",\"member date of birth\",\"member sex\",\"submit date\",\"year\",\"month\",\"quarter\"],\"query_patterns\":[\"claim-level financial analysis\",\"client-level analysis\",\"drug and therapy class performance\",\"manufacturer-level insights\",\"revenue per script and GDR metrics\",\"line-of-business tracking\",\"daily and monthly trend analysis\",\"rate analysis\",\"pharmacy performance\",\"member usage analysis\"],\"not_suitable_for\":[\"budget planning\",\"forecast generation\",\"ledger-level summaries\"],\"grain\":\"claim_transaction\",\"temporal\":\"daily via submit_date\"}"
  },
  {
    "llm_context": "{\"table_name\":\"prd_optumrx_orxfdmprdsa.rag.actuals_vs_forecast_analysis\",\"purpose\":\"Ledger-level financial data for actuals, forecast, and budget analysis. Aggregated financial metrics for forecast analysis and planning at LOB level.\",\"key_measures\":[\"revenue\",\"IOI\",\"total membership\",\"unadjusted scripts\",\"adjusted scripts\",\"30-day scripts\",\"90-day scripts\",\"amount or count\",\"expense\",\"volume\"],\"key_dimensions\":[\"ledger type\",\"line of business\",\"product category\",\"product subcategory level 1\",\"product subcategory level 2\",\"transaction date\",\"year\",\"month\",\"quarter\"],\"query_patterns\":[\"actuals vs forecast\",\"budget comparison\"],\"not_suitable_for\":[\"claim-level analysis\",\"daily granularity\",\"client-specific analysis\"],\"grain\":\"aggregated_metrics\",\"temporal\":\"monthly/quarterly/yearly via transaction_date\"}"
  }
]
