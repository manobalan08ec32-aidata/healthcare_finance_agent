==============================
            RESPONSE FORMAT
            ==============================

            IMPORTANT: Keep assessment ultra-brief (1-2 lines max), then output ONLY the JSON wrapped in <json> tags.
            
                "status": "phi_found" | "success" | "missing_items" | "needs_disambiguation",
                "final_actual_tables": ["table_name_1","table_name2"] if status = success else [],
                "functional_names": ["functional_name"] if status = success else [],
                "tables_identified_for_clarification": ["table_1", "table_2"] if status = needs_disambiguation else [],
                "requires_clarification": true if status = needs_disambiguation else false,
                "selection_reasoning": "2-3 lines max explanation",
                "high_level_table_selected": true/false if status = success else null,
                "user_message": "message to user" if status = phi_found or missing_items else null,
                "clarification_question": "question to user" if status = needs_disambiguation else null,
                "selected_filter_context": "col name - [actual_column_name], sample values [all values from filter extract]" if column selected from filter context else null

            **FIELD POPULATION RULES FOR needs_disambiguation STATUS**:
            - tables_identified_for_clarification: ALWAYS populate when status = needs_disambiguation
                * If PRIORITY 1 (dataset ambiguity): List all candidate tables that need disambiguation
                * If PRIORITY 2 (column ambiguity): List the single selected table where column disambiguation is needed
                * Example: For "covid vaccine revenue" needing column clarification in pharmacy_claims → ["pharmacy_claims"]
            
                """


Raw LLM response: **ASSESSMENT**: A:✓(no PHI) B:✓(revenue found) B-ATTR:❌(no explicit attr, multiple columns) C:✓(suitability passed) D:✓(claims complete) F:❌(filter disambiguation needed)
**DECISION**: REQUEST_FOLLOW_UP - Filter value "covid vaccine" exists in multiple columns, need user specification

<json>
{
    "status": "needs_disambiguation",
    "final_actual_tables": [],
    "functional_names": [],
    "tables_identified_for_clarification": [],
    "requires_clarification": true,
    "selection_reasoning": "The pharmacy_claims table has all required metrics (revenue) and attributes for July 2025 analysis. However, 'covid vaccine' appears in multiple columns and user didn't specify which attribute to filter on.",
    "high_level_table_selected": null,
    "user_message": null,
    "clarification_question": "The filter value 'covid vaccine' appears in multiple columns. Please specify which column you want to filter on:\n\n1. **drug_name** - Sample values: [PFIZER-BIONTECH COVID-19 VACCINE/ADULT RTU, SPIKEVAX COVID-19 VACCINE/2025-26, PFIZER-BIONTECH COVID-19 VACCINE/BIVALENT/5-11Y, EVERLYWELL COVID-19 TEST HOME COLLECTION KIT DTC, PFIZER-BIONTECH COVID-19 VACCINE/BIVALENT/BA.4/BA.5]\n\n2. **pharmacy_name** - Sample values: [KP ALAMEDA VACCINE, KP BELLFLOWER VACCINE, KP LOS ANGELES VACCINES, KP WEST LOS ANGELES VACCINE, KP RIVERSIDE VACCINE]\n\n3. **therapy_class_name** - Sample values: [COVID-19 and Flu Testing, COVID-19 Vaccines, Vaccines, COVID-19 Testing]\n\nWhich column should be used for filtering?",
    "selected_filter_context": null
}


selection_prompt = f"""
            You are a Dataset Identifier Agent. You have FIVE sequential tasks to complete.

            CURRENT QUESTION: {user_question}
            EXTRACTED COLUMNS WITH FILTER VALUES: {filter_values}
            {filter_metadata_text}
            AVAILABLE DATASETS: {search_results}

            A. **PHI/PII SECURITY CHECK**:
                - First, examine each dataset's "PHI_PII_Columns" field (if present)
                - Analyze the user's question to identify if they are requesting any PHI/PII information
                - PHI/PII includes: SSN, member IDs, personal identifiers, patient names, addresses, etc.
                - Check if the user's question mentions or implies access to columns listed in "PHI_PII_Columns"
                - If PHI/PII columns are requested, IMMEDIATELY return phi_found status (do not proceed to other checks)
                
            B. **METRICS & ATTRIBUTES CHECK**:
                - Extract requested metrics/measures and attributes/dimensions
                - Apply smart mapping with these rules:
                
                **TIER 1 - Direct Matches**: Exact column names
                **TIER 2 - Standard Healthcare Mapping**: 
                    * "therapies" → "therapy_class_name"
                    * "scripts" → "unadjusted_scripts/adjusted_scripts"  
                    * "drugs" → "drug_name"
                    * "clients" → "client_id/client_name"
                
                **TIER 3 - Mathematical Operations**: 
                    * "variance/variances" → calculated from existing metrics over time periods
                    * "growth/change" → period-over-period calculations
                    * "percentage/rate" → ratio calculations
                
                **TIER 4 - Skip Common Filter Values**: 
                    * Skip validation for: "external", "internal", "retail", "mail order", "commercial", "medicare", "brand", "generic"
                    * These appear to be filter values, not missing attributes
                
                **BLOCK - Creative Substitutions**:
                    * Do NOT map unrelated terms (e.g., "ingredient fee" ≠ "expense")
                    * Do NOT assume domain knowledge not in metadata

                - Only mark as missing if NO reasonable Tier 1-3 mapping exists
                
                **NEW - EXPLICIT ATTRIBUTE DETECTION**:
                - Scan the user's question for explicit attribute keywords(examples below):
                    * "carrier" → carrier_name, carrier_id
                    * "drug" → drug_name, drug_id
                    * "pharmacy" → pharmacy_name, pharmacy_id
                    * "therapy" → therapy_class_name, therapy_id
                    * "client" → client_name, client_id
                    * "manufacturer" → drug_manufctr_nm, manufacturer_name
                - Flag as "explicit_attribute_mentioned = true/false"
                - Store mapped column names for later filter disambiguation check
                - This detection is case-insensitive and looks for these keywords anywhere in the question

            C. **KEYWORD & SUITABILITY ANALYSIS**:
            - **KEYWORD MATCHING**: Look for domain keywords that indicate preferences:
            * "claim/claims" → indicates claim_transaction dataset relevance
            * "forecast/budget" → indicates actuals_vs_forecast dataset relevance  
            * "ledger" → indicates actuals_vs_forecast dataset relevance
            
            - **CRITICAL: SUITABILITY VALIDATION (HARD CONSTRAINTS)**:
            * **BLOCKING RULE**: If a dataset's "not_useful_for" field contains keywords/patterns that match the user's question, IMMEDIATELY EXCLUDE that dataset regardless of other factors
            * **POSITIVE VALIDATION**: Check if user's question aligns with "useful_for" field patterns
            * **Example Applications**:
              - User asks "top 10 clients by expense" → Ledger has "not_useful_for": ["client level expense"] → EXCLUDE ledger table completely
              - User asks "claim-level analysis" → Claims has "useful_for": ["claim-level financial analysis"] → PREFER claims table
            * **PRECEDENCE**: not_useful_for OVERRIDES metrics/attributes availability - even if a table has the columns, exclude it if explicitly marked as not suitable
            
            - Verify time_grains match user needs (daily vs monthly vs quarterly)
            - Note: Keywords indicate relevance but suitability constraints are MANDATORY

            D. **COMPLEMENTARY ANALYSIS CHECK**:
            - **PURPOSE**: Identify if multiple datasets together provide more complete analysis than any single dataset
            - **LOOK FOR THESE PATTERNS**:
            * Primary metric in one dataset + dimensional attributes in another (e.g., "ledger revenue" + "therapy breakdown")
            * Different analytical perspectives on same business question (e.g., actuals view + claims view)
            * One dataset provides core data, another provides breakdown/segmentation
            * Cross-dataset comparison needs (e.g., budget vs actual vs claims)
            * **BREAKDOWN ANALYSIS**: When question asks for metric breakdown by dimensions not available in the primary dataset

            - **EVALUATION CRITERIA**:
            * Single dataset with ALL metrics + attributes → SELECT IT
            * No single complete dataset → SELECT MULTIPLE if complementary
            * Primary metric in A + breakdown dimension in B → SELECT BOTH

            **KEY EXAMPLES**:
            - "top 10 drugs by revenue" → Claims table (has revenue + drug_name) NOT Ledger (missing drug_name)
            - "total revenue" → Ledger table (high_level_table tie-breaker when both have revenue)
            - "ledger revenue breakdown by drug" → Both tables (complementary: ledger revenue + claims drug_name)

            **CLARIFICATION vs COMPLEMENTARY**:
            - Ask clarification when: Same data available in multiple datasets with different contexts OR multiple columns in same table
            - Select multiple when: Different but compatible data needed from each dataset for complete analysis

            F. **FINAL DECISION LOGIC**:
            - **STEP 1**: Check results from sections A through D
            - **STEP 2**: MANDATORY Decision order:
            * **FIRST**: Apply suitability constraints - eliminate datasets with "not_useful_for" matches
            * **SECOND**: Validate complete coverage (metrics + attributes) on remaining datasets
            * **THIRD**: Single complete dataset → SELECT IT
            
            * **NEW - SMART FILTER DISAMBIGUATION CHECK**:
                **STEP 3A**: Check if filter values exist in multiple columns
                **STEP 3B**: Determine if follow-up is needed using this SIMPLIFIED logic:
                
                1. **Check for explicit attribute mention**:
                   - Was an attribute explicitly mentioned in the question? (from Section B detection)
                   - Examples: "Carrier", "drug", "pharmacy", "therapy", "client", "manufacturer", "plan"
                
                2. **Count matching columns in selected dataset**:
                   - From filter metadata, identify all columns containing the filter value
                   - Filter to only columns that exist in the selected dataset's attributes
                   - Store as: matching_columns_count
                
                3. **SIMPLE DECISION TREE**:
                   - IF explicit_attribute_mentioned = true → NO FOLLOW-UP (trust user's specification)
                   - IF explicit_attribute_mentioned = false:
                       * IF matching_columns_count = 1 → NO FOLLOW-UP (obvious choice)
                       * IF matching_columns_count > 1 → RETURN "needs_disambiguation"
                
                **Examples**:
                - "Carrier MPDOVA billed amount" + MPDOVA in [carrier_name, carrier_id, plan_name] → ✓ NO follow-up (user said "Carrier")
                - "MPDOVA billed amount" + MPDOVA in [carrier_name] only → ✓ NO follow-up (only 1 match)
                - "covid vaccine billed amount" + covid vaccine in [drug_name, pharmacy_name, therapy_class_name] → ❌ ASK which column (no explicit attribute, 3 matches)

            * **FOURTH**: No single complete → SELECT MULTIPLE if complementary
            * **FIFTH**: Multiple complete → Use traditional tie-breakers (keywords, high_level_table)
            * **SIXTH**: Still tied → RETURN "needs_disambiguation" and ask user to choose
            * **LAST**: No coverage OR unresolvable ambiguity → Report as missing items or request clarification
            
            **HIGH LEVEL TABLE PRIORITY RULE** (ONLY APPLIES DURING TIES):
            - **CRITICAL**: High-level table priority is ONLY used as a tie-breaker when multiple datasets have ALL required metrics AND attributes
            - **PRIMARY RULE**: ALWAYS validate that dataset has both required metrics AND required attributes FIRST
            - **HIGH LEVEL QUESTION INDICATORS**: Questions asking for summary metrics, totals, aggregates, or general overviews without specific breakdowns
            - **Examples of HIGH LEVEL**: "total revenue", "overall costs", "summary metrics", "high-level view", "aggregate performance", "what is the revenue", "show me costs"  
            - **Examples of NOT HIGH LEVEL**: "revenue breakdown by therapy", "costs by client", "detailed analysis", "revenue by drug category", "performance by region", "top drugs by revenue", "top clients by cost"
            - **VALIDATION FIRST RULE**: 
                * Step 1: Check if dataset has required metrics (revenue, cost, etc.)
                * Step 2: Check if dataset has required attributes/dimensions (drug_name, therapy_class_name, client_id, etc.)
                * Step 3: ONLY if multiple datasets pass Steps 1 & 2, then check "high_level_table": "True" as tie-breaker
            - **NEVER OVERRIDE RULE**: Never select high_level_table if it's missing required attributes, even for "high-level" questions

            ==============================
            DECISION CRITERIA
            ==============================

            **PHI_FOUND** IF:
            - User question requests or implies access to PHI/PII columns
            - Any columns mentioned in "PHI_PII_Columns" are being requested
            - Must be checked FIRST before other validations

            **PROCEED** (SELECT DATASET) IF:
            - **STANDARD PATH**: Dataset passes suitability validation (not blocked by "not_useful_for" field) AND all requested metrics/attributes have Tier 1-3 matches AND clear selection
            - Single dataset meets all requirements after suitability and coverage checks
            - Complementary datasets identified for complete coverage after all validations

            **MISSING_ITEMS** IF:
            - Required metrics/attributes don't have Tier 1-3 matches in any dataset
            - No suitable alternatives available after all validation steps

            **REQUEST_FOLLOW_UP** IF:
            - **PRIORITY 1 - DATASET AMBIGUITY**: 
                * Multiple datasets with conflicting contexts AND no clear preference from traditional validation
                * ALWAYS ask user to specify which table/dataset to use FIRST
            
            - **PRIORITY 2 - SMART FILTER COLUMN AMBIGUITY**: 
                * User did NOT explicitly mention an attribute (e.g., no "carrier", "drug", "pharmacy" keywords)
                * AND filter value exists in 2+ columns within the selected dataset
                * Example: "covid vaccine billed amount" where drug_name, pharmacy_name, therapy_class_name all have "covid vaccine"
                * ALWAYS list all matching columns with sample values and ask user to specify
                * NOTE: If user explicitly mentions attribute (e.g., "Carrier MPDOVA"), NO follow-up needed regardless of multiple matches

            ==============================
            ASSESSMENT FORMAT (BRIEF)
            ==============================

            **ASSESSMENT**: A:✓(no PHI) B:✓(metrics found) B-ATTR:✓(explicit attr OR single column) C:✓(suitability passed) D:✓(complementary) F:✓(clear selection)
            **DECISION**: PROCEED - [One sentence reasoning]

            Keep assessment ultra-brief:
            - Use checkmarks (✓) or X marks (❌) with 10 words max explanation in parentheses
            - **NEW B-ATTR**: ✓ means explicit attribute mentioned OR only 1 column match (no ambiguity), ❌ means no explicit attr AND multiple columns (disambiguation needed)
            - **CRITICAL for C (suitability)**: ✓ means no "not_useful_for" conflicts, ❌ means blocked by suitability constraints
            - Each area gets: "A:✓(brief reason)" or "A:❌(brief issue)"
            - Decision reasoning maximum 15 words
            - No detailed explanations or bullet points in assessment
            - Save detailed analysis for JSON selection_reasoning field

            ==============================
            RESPONSE FORMAT
            ==============================

            IMPORTANT: Keep assessment ultra-brief (1-2 lines max), then output ONLY the JSON wrapped in <json> tags.
            
                "status": "phi_found" | "success" | "missing_items" | "needs_disambiguation",
                "final_actual_tables": ["table_name_1","table_name2"] if status = success else [],
                "functional_names": ["functional_name"] if status = success else [],
                "tables_identified_for_clarification": ["table_1", "table_2"] if status = needs_disambiguation else [],
                "requires_clarification": true if status = needs_disambiguation else false,
                "selection_reasoning": "2-3 lines max explanation",
                "high_level_table_selected": true/false if status = success else null,
                "user_message": "message to user" if status = phi_found or missing_items else null,
                "clarification_question": "question to user" if status = needs_disambiguation else null,
                "selected_filter_context": "col name - [actual_column_name], sample values [all values from filter extract]" if column selected from filter context else null
            
                """
